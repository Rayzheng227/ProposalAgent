import arxiv
from crossref.restful import Works
from langchain_community.tools import TavilySearchResults
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from typing import TypedDict, List, Dict, Any
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet
import json
import os
from datetime import datetime
import logging
from prompts import *
import fitz

"""
APIçš„é…ç½®â€”â€” 
TODO:åŒæ­¥åˆ°ç½‘ä¸Šå‰è®°å¾—ä¿®æ”¹ï¼
"""
base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
DASHSCOPE_API_KEY = "sk-eafdf8e1d0fb4717a883c87788e76182"

Tavily_API_KEY = "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"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # è¾“å‡ºåˆ°æ§åˆ¶å°
    ]
)

class ProposalState(TypedDict):
    """å®šä¹‰Proposalç”Ÿæˆè¿‡ç¨‹ä¸­çš„çŠ¶æ€"""
    research_field: str
    query: str
    arxiv_papers: List[Dict]
    web_search_results: List[Dict]
    background: str
    objectives: str
    methodology: str
    timeline: str
    expected_outcomes: str
    final_proposal: str # Potentially redundant, consider removing if final_report_markdown is comprehensive
    messages: List[Any]
    research_plan: str
    available_tools: List[Dict]  # å­˜å‚¨å¯ç”¨å·¥å…·ä¿¡æ¯
    execution_plan: List[Dict]  # å¯æ‰§è¡Œçš„è®¡åˆ’
    execution_memory: List[Dict]  # å·²ç»æ‰§è¡Œçš„è®°å¿†
    current_step: int  # å½“å‰æ‰§è¡Œçš„æ­¥éª¤
    max_iterations: int  # æœ€å¤§è¿­ä»£æ¬¡æ•°
    introduction: str
    literature_review: str
    research_design: str
    timeline_plan: str # Note: This might be redundant if CONCLUSION_PROMPT handles timeline
    expected_results: str # Note: This might be redundant if CONCLUSION_PROMPT handles expected outcomes
    reference_list: List[Dict]  # ç»Ÿä¸€çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨
    ref_counter: int  # å‚è€ƒæ–‡çŒ®è®¡æ•°å™¨
    final_references: str  # æœ€ç»ˆçš„å‚è€ƒæ–‡çŒ®éƒ¨åˆ†
    conclusion: str # æ–°å¢ç»“è®ºå­—æ®µ
    final_report_markdown: str # æ–°å¢æœ€ç»ˆæŠ¥å‘ŠMarkdownå†…å®¹å­—æ®µ


@tool
def search_arxiv_papers_tool(query: str, max_results: int = 10, Download: bool = True) -> List[Dict]:
    """æœç´¢å¹¶ä¸‹è½½ArXivè®ºæ–‡çš„å·¥å…·
    
    Args:
        query: æœç´¢å…³é”®è¯
        max_results: æœ€å¤§ç»“æœæ•°é‡ï¼Œé»˜è®¤5ç¯‡
        Download: æ˜¯å¦ä¸‹è½½PDFæ–‡ä»¶
    
    Returns:
        åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        ä»¥åŠå­˜å‚¨åœ¨Papersç›®å½•ä¸‹çš„å‚è€ƒæ–‡çŒ®
    """
    logging.info(f"åœ¨arxivä¸Šæœç´¢:{query}")
    
    try:
        client = arxiv.Client()
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        
        papers_dir = "./Papers"
        if not os.path.exists(papers_dir):
            os.makedirs(papers_dir)
        
        papers = []
        for paper in client.results(search):
            paper_info = {
                "title": paper.title,
                "authors": [author.name for author in paper.authors],
                "summary": paper.summary[:300] + "...",  # æˆªæ–­æ‘˜è¦
                "published": paper.published.strftime("%Y-%m-%d"),
                "pdf_url": paper.pdf_url,
                "categories": paper.categories,
                "arxiv_id": paper.entry_id.split('/')[-1]
            }
            
            if Download:
                try:
                    # ä¸‹è½½PDF - æ”¹è¿›æ–‡ä»¶åå¤„ç†å’Œé”™è¯¯å¤„ç†
                    logging.info(f"æ­£åœ¨ä¸‹è½½è®ºæ–‡ï¼š{paper.title[:50]}...")
                    
                    # æ›´å®‰å…¨çš„æ–‡ä»¶åå¤„ç†
                    import re
                    safe_title = re.sub(r'[^\w\s-]', '', paper.title)  # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
                    safe_title = re.sub(r'[-\s]+', '-', safe_title)    # æ›¿æ¢ç©ºæ ¼å’Œå¤šä¸ªè¿å­—ç¬¦
                    safe_title = safe_title.strip('-')[:40]             # é™åˆ¶é•¿åº¦å¹¶ç§»é™¤é¦–å°¾è¿å­—ç¬¦
                    
                    if not safe_title:  # å¦‚æœæ ‡é¢˜å¤„ç†åä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
                        safe_title = "paper"
                    
                    filename = f"{paper_info['arxiv_id']}_{safe_title}.pdf"
                    full_path = os.path.join(papers_dir, filename)
                    
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                    if os.path.exists(full_path):
                        logging.info(f"è®ºæ–‡å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {filename}")
                        paper_info["local_pdf_path"] = full_path
                    else:
                        # ä½¿ç”¨æ›´ç¨³å®šçš„ä¸‹è½½æ–¹æ³•
                        import time
                        time.sleep(5)  # å¢åŠ ä¸‹è½½é—´éš”æ—¶é—´ï¼Œä¾‹å¦‚5ç§’ï¼Œä»¥å‡å°‘æœåŠ¡å™¨å‹åŠ›
                        
                        paper.download_pdf(dirpath=papers_dir, filename=filename)
                        
                        # éªŒè¯ä¸‹è½½æ˜¯å¦æˆåŠŸ
                        if os.path.exists(full_path) and os.path.getsize(full_path) > 0:
                            paper_info["local_pdf_path"] = full_path
                            logging.info(f"âœ… æˆåŠŸä¸‹è½½: {filename}")
                        else:
                            paper_info["local_pdf_path"] = None
                            logging.warning(f"âŒ ä¸‹è½½å¤±è´¥æˆ–æ–‡ä»¶ä¸ºç©º: {filename}")
                        
                except Exception as e:
                    paper_info["local_pdf_path"] = None
                    logging.warning(f"âŒ ä¸‹è½½è®ºæ–‡å¤±è´¥: {paper.title[:50]}... - é”™è¯¯: {str(e)}")
                    
                    # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œå°è¯•è®°å½•æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                    error_str = str(e).lower()
                    if "timeout" in error_str:
                        logging.warning("å¯èƒ½çš„ç½‘ç»œè¶…æ—¶é—®é¢˜ã€‚")
                    elif "permission" in error_str or "403" in error_str or "forbidden" in error_str:
                        logging.warning("å¯èƒ½çš„æƒé™é—®é¢˜æˆ–è¯·æ±‚è¢«ç¦æ­¢ (403 Forbidden)ã€‚è¿™å¯èƒ½æ˜¯ç”±äºè¯·æ±‚é¢‘ç‡è¿‡é«˜ã€‚")
                    elif "not found" in error_str or "404" in error_str:
                        logging.warning("PDFæ–‡ä»¶å¯èƒ½ä¸å­˜åœ¨ (404 Not Found)ã€‚")
                    elif "bad gateway" in error_str or "502" in error_str:
                        logging.warning("æœåŠ¡å™¨ç«¯é”™è¯¯ (502 Bad Gateway)ã€‚è¿™å¯èƒ½æ˜¯ArXivæœåŠ¡å™¨çš„ä¸´æ—¶é—®é¢˜ã€‚")
            
            papers.append(paper_info)
            
            # é™åˆ¶å¤„ç†æ•°é‡ï¼Œé¿å…è¿‡å¤šè¯·æ±‚
            if len(papers) >= max_results:
                break

        logging.info(f"âœ… ArXivæœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        successful_downloads = len([p for p in papers if p.get("local_pdf_path")])
        logging.info(f"ğŸ“„ æˆåŠŸä¸‹è½½ {successful_downloads} ä¸ªPDFæ–‡ä»¶")
        
        return papers

    except Exception as e:
        logging.error(f"âŒ ArXivæœç´¢å¤±è´¥: {str(e)}")
        return [{"error": f"ArXivæœç´¢å¤±è´¥: {str(e)}"}]


@tool
def search_web_content_tool(query: str) -> List[Dict]:
    """ä½¿ç”¨Tavilyæœç´¢ç½‘ç»œå†…å®¹çš„å·¥å…·
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨
    """
    logging.info(f"æ­£åœ¨ç½‘ç»œæœç´¢:{query}")
    
    try:
        os.environ["TAVILY_API_KEY"] = Tavily_API_KEY
        tavily_tool = TavilySearchResults(
            max_results=5,
            search_depth="advanced",
            include_answer=True,
            include_raw_content=True
        )
        
        results = tavily_tool.invoke({"query": query})
        return results
    
    except Exception as e:
        return [{"error": f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}"}]


@tool
def search_crossref_papers_tool(query: str, max_results: int = 5) -> List[Dict]:
    """ä½¿ç”¨ CrossRef æœç´¢è®ºæ–‡å…ƒæ•°æ®çš„å·¥å…·

    Args:
        query: å…³é”®è¯æˆ–ä¸»é¢˜
        max_results: è¿”å›ç»“æœæ•°é‡ä¸Šé™ï¼ˆé»˜è®¤5ï¼‰

    Returns:
        åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
    """
    logging.info(f"åœ¨crossrefä¸Šæœç´¢:{query}")
    
    try:
        works = Works()
        search = works.query(query).sort('relevance')

        results = []
        for i, item in enumerate(search):
            if i >= max_results:
                break

            paper_info = {
                "title": item.get("title", ["No title"])[0],
                "authors": [
                    f"{author.get('given', '')} {author.get('family', '')}".strip()
                    for author in item.get("author", [])
                ],
                "doi": item.get("DOI", "N/A"),
                "published": "-".join(str(d) for d in item.get("issued", {}).get("date-parts", [[None]])[0]),
                "publisher": item.get("publisher", "N/A"),
                "journal": item.get("container-title", ["N/A"])[0],
                "url": item.get("URL", "N/A")
            }

            results.append(paper_info)

        return results

    except Exception as e:
        return [{"error": f"CrossRef æœç´¢å¤±è´¥: {str(e)}"}]


@tool
def summarize_pdf(path: str, max_chars: int = 10000) -> Dict:
    """æ€»ç»“PDFæ–‡ä»¶å†…å®¹çš„å·¥å…·
    
    Args:
        path: PDFæ–‡ä»¶è·¯å¾„
        max_chars: æœ€å¤§å­—ç¬¦æ•°é™åˆ¶ï¼Œé»˜è®¤10000
        
    Returns:
        åŒ…å«æ‘˜è¦å’Œæºæ–‡æœ¬ç‰‡æ®µçš„å­—å…¸
    """
    logging.info(f"è°ƒç”¨å·¥å…·ï¼šsummarize_pdf:{path}")
    
    try:
        # 1. æ‰“å¼€å¹¶æå– PDF æ–‡æœ¬
        doc = fitz.open(path)
        full_text = ""
        for page in doc:
            full_text += page.get_text()
            if len(full_text) > max_chars:
                full_text = full_text[:max_chars]
                break

        doc.close()

        if not full_text.strip():
            logging.warning(f"PDF æ–‡ä»¶ '{path}' ä¸­æœªæ‰¾åˆ°å¯ç”¨æ–‡æœ¬")
            return {"error": "PDF æ–‡ä»¶ä¸­æœªæ‰¾åˆ°å¯ç”¨æ–‡æœ¬"}

        # 2. æ„é€ æ‘˜è¦æç¤º
        prompt = f"""
        You are an academic assistant specializing in research paper analysis.
        Summarize the following academic text into a comprehensive but concise analysis (around 300-400 words in Chinese).
        Focus on:
        1. ç ”ç©¶ç›®æ ‡å’Œé—®é¢˜
        2. ä¸»è¦æ–¹æ³•è®º
        3. æ ¸å¿ƒå‘ç°å’Œç»“è®º
        4. ç ”ç©¶è´¡çŒ®å’Œæ„ä¹‰
        
        è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä½¿ç”¨å­¦æœ¯åŒ–çš„è¯­è¨€ã€‚

        Text:
        \"\"\"
        {full_text}
        \"\"\"
        """

        # 3. è°ƒç”¨è¯­è¨€æ¨¡å‹
        from langchain_openai import ChatOpenAI
        llm = ChatOpenAI(
            temperature=0, 
            model="qwen-plus", 
            base_url=base_url, 
            api_key=DASHSCOPE_API_KEY
        )

        logging.info(f"æ­£åœ¨ä¸ºPDFæ–‡ä»¶ '{path}' ç”Ÿæˆæ‘˜è¦...")
        response = llm.invoke([HumanMessage(content=prompt)])
        logging.info(f"æ‘˜è¦: {response.content.strip()}")
        return {
            "summary": response.content.strip(),
            "source_excerpt": full_text[:500] + "...",  # è¿”å›å‰ 500 å­—ç”¨äºä¸Šä¸‹æ–‡å‚è€ƒ
            "total_length": len(full_text)
        }

    except Exception as e:
        return {"error": f"PDF æ‘˜è¦å¤±è´¥: {str(e)}"}


class ProposalAgent:
    def __init__(self):
        """åˆå§‹åŒ–ProposalAgent"""
        self.llm = ChatOpenAI(
            api_key= DASHSCOPE_API_KEY,
            model="qwen-plus",
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            temperature=0
        )
        
        # è®¾ç½®Tavily APIå¯†é’¥
        os.environ["TAVILY_API_KEY"] = Tavily_API_KEY

        self.tools = [search_arxiv_papers_tool, search_web_content_tool, search_crossref_papers_tool, summarize_pdf]
        self.tools_description = self.load_tools_description()
        self.agent_with_tools = create_react_agent(self.llm, self.tools)
        self.workflow = self._build_workflow()

    
    def load_tools_description(self) -> List[Dict]:
        """ä»JSONæ–‡ä»¶åŠ è½½å·¥å…·æè¿°"""
        try:
            with open('tools.json', 'r', encoding='utf-8') as f:
                tools_description = json.load(f)
            return tools_description
        except FileNotFoundError:
            print("è­¦å‘Š: tools.json æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å·¥å…·æè¿°")
            return []
        except json.JSONDecodeError:
            print("è­¦å‘Š: tools.json æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤å·¥å…·æè¿°")
            return []
        

    def get_tools_info_text(self) -> str:
        """å°†å·¥å…·ä¿¡æ¯è½¬æ¢ä¸ºæ–‡æœ¬æè¿°"""
        if not self.tools_description:
            return "æš‚æ— å¯ç”¨å·¥å…·ä¿¡æ¯"
        
        tools_text = "å¯ç”¨å·¥å…·åˆ—è¡¨:\n\n"
        for tool_info in self.tools_description:
            func_info = tool_info.get("function", {})
            name = func_info.get("name", "æœªçŸ¥å·¥å…·")
            description = func_info.get("description", "æ— æè¿°")
            
            tools_text += f"ğŸ”§ **{name}**\n"
            tools_text += f"   æè¿°: {description}\n"
            
            # æ·»åŠ å‚æ•°ä¿¡æ¯
            params = func_info.get("parameters", {}).get("properties", {})
            if params:
                tools_text += f"   å‚æ•°:\n"
                for param_name, param_info in params.items():
                    param_desc = param_info.get("description", "æ— æè¿°")
                    param_type = param_info.get("type", "æœªçŸ¥ç±»å‹")
                    required = param_name in func_info.get("parameters", {}).get("required", [])
                    required_text = "å¿…éœ€" if required else "å¯é€‰"
                    tools_text += f"     - {param_name} ({param_type}, {required_text}): {param_desc}\n"
            
            tools_text += "\n"
        
        return tools_text
    

    def create_master_plan_node(self, state: ProposalState) -> ProposalState:
        """é¦–å…ˆåŸºäºé—®é¢˜å»åˆ›å»ºä¸€ä¸ªæ€»ä½“çš„è§„åˆ’(ä¸åŒäºProposal)"""
        research_field = state["research_field"]

        tools_info = self.get_tools_info_text()

        master_planning_prompt = master_plan_instruction.format(
            research_field=research_field,
            tools_info=tools_info
        )
        logging.info(f"ğŸ¤– Agentæ­£åœ¨ä¸º '{research_field}' åˆ¶å®šæ€»ä½“ç ”ç©¶è®¡åˆ’...")
        response = self.llm.invoke([HumanMessage(content=master_planning_prompt)])
        
        state["research_plan"] = response.content
        state["available_tools"] = self.tools_description
        state["execution_memory"] = []
        state["current_step"] = 0
        state["max_iterations"] = 10

        logging.info("âœ… æ€»ä½“ç ”ç©¶è®¡åˆ’åˆ¶å®šå®Œæˆ")
        logging.info(f"ç ”ç©¶è®¡åˆ’å†…å®¹: {state['research_plan']}...")

        return state
    


    def plan_analysis_node(self, state: ProposalState) -> ProposalState:
        """è§£æç ”ç©¶è®¡åˆ’,ç”Ÿæˆå¯æ‰§è¡Œæ­¥éª¤"""
        research_field = state["research_field"]
        research_plan = state["research_plan"]
        tools_info = self.get_tools_info_text()
        execution_memory = state.get("execution_memory", [])


        memory_text = ""
        if execution_memory:
            memory_text = "\n\næ‰§è¡Œå†å²:\n"
            for step in execution_memory:
                description = step.get('description', 'æœªçŸ¥æ­¥éª¤')
                result = step.get('result', 'æ— ç»“æœ')
                success = step.get('success', False)
                status = "æˆåŠŸ" if success else "å¤±è´¥"
                memory_text += f"- {description}: {status} - {result[:100]}...\n"

        
        # é¦–å…ˆè®©Agentåˆ†æè®¡åˆ’ï¼Œç¡®å®šæ£€ç´¢ç­–ç•¥
        plan_analysis_prompt = EXECUTION_PLAN_PROMPT.format(
            research_field=research_field,
            research_plan=research_plan,
            tools_info=tools_info,
            memory_text=memory_text
        )
        logging.info("ğŸ” Agentæ­£åœ¨åˆ†æè®¡åˆ’å¹¶ç”Ÿæˆæ‰§è¡Œæ­¥éª¤...")
        response = self.llm.invoke([HumanMessage(content=plan_analysis_prompt)])
        logging.info("ç”Ÿæˆè®¡åˆ’", response.content)
        try:
            # è§£æJSONå“åº”
            response_text = response.content.strip()
           # å¦‚æœå“åº”åŒ…å«```jsonï¼Œåˆ™æå–JSONéƒ¨åˆ†
            if "```json" in response_text:
                start = response_text.find("```json") + 7
                end = response_text.find("```", start)
                if end != -1:
                    response_text = response_text[start:end].strip()
            elif "```" in response_text:
                start = response_text.find("```") + 3
                end = response_text.find("```", start)
                if end != -1:
                    response_text = response_text[start:end].strip()
            
            plan_data = json.loads(response_text)
            state["execution_plan"] = plan_data.get("steps", [])
        except json.JSONDecodeError:
            logging.error("æ— æ³•è§£ææ‰§è¡Œè®¡åˆ’JSONï¼Œä½¿ç”¨é»˜è®¤è®¡åˆ’")
            logging.error(f"åŸå§‹å“åº”: {response.content[:500]}...")
            # é»˜è®¤æ‰§è¡Œè®¡åˆ’
            state["execution_plan"] = [
                {
                    "step_id": 1,
                    "action": "search_arxiv_papers",
                    "parameters": {"query": research_field, "max_results": 5},
                    "description": f"æœç´¢ArXivä¸Šå…³äº{research_field}çš„è®ºæ–‡",
                    "expected_outcome": "æ‰¾åˆ°ç›¸å…³çš„å­¦æœ¯è®ºæ–‡"
                }
            ]
        
        logging.info(f"âœ… ç”Ÿæˆäº† {len(state['execution_plan'])} ä¸ªæ‰§è¡Œæ­¥éª¤")

        return state
    

    def execute_step_node(self, state: ProposalState) -> ProposalState:
        """æ‰§è¡Œå½“å‰æ­¥éª¤"""
        execution_plan = state.get("execution_plan", [])
        current_step = state.get("current_step", 0)
        execution_memory = state.get("execution_memory", [])
        
        if current_step >= len(execution_plan):
            logging.info("æ‰€æœ‰æ­¥éª¤å·²æ‰§è¡Œå®Œæˆ")
            return state
        
        current_action = execution_plan[current_step]
        action_name = current_action.get("action")
        parameters = current_action.get("parameters", {})
        description = current_action.get("description", "")
        
        logging.info(f"ğŸš€ æ‰§è¡Œæ­¥éª¤ {current_step + 1}: {description}")
        
        # æ‰§è¡Œå¯¹åº”çš„å·¥å…·
        result = None
        try:
            if action_name == "search_arxiv_papers":
                result = search_arxiv_papers_tool.invoke(parameters)
                # å°†ç»“æœä¿å­˜åˆ°çŠ¶æ€ä¸­
                if isinstance(result, list) and len(result) > 0:
                    state["arxiv_papers"].extend(result)
                    
            elif action_name == "search_web_content":
                result = search_web_content_tool.invoke(parameters)
                # å°†ç»“æœä¿å­˜åˆ°çŠ¶æ€ä¸­
                if isinstance(result, list) and len(result) > 0:
                    state["web_search_results"].extend(result)
                    
            elif action_name == "search_crossref_papers":
                result = search_crossref_papers_tool.invoke(parameters)
                # å°†ç»“æœä¿å­˜åˆ°çŠ¶æ€ä¸­
                if isinstance(result, list) and len(result) > 0:
                    state["web_search_results"].extend(result)
                    
            elif action_name == "summarize_pdf":
                # æ”¹è¿›PDFæ‘˜è¦çš„è·¯å¾„å¤„ç†
                pdf_path = parameters.get("path", "")
                
                # å¦‚æœæ²¡æœ‰æŒ‡å®šå…·ä½“è·¯å¾„ï¼Œå°è¯•æ‰¾åˆ°å·²ä¸‹è½½çš„PDF
                if not pdf_path or not os.path.exists(pdf_path):
                    # æŸ¥æ‰¾å·²ä¸‹è½½çš„PDFæ–‡ä»¶
                    available_pdfs = []
                    for paper in state.get("arxiv_papers", []):
                        if paper.get("local_pdf_path") and os.path.exists(paper["local_pdf_path"]):
                            available_pdfs.append(paper["local_pdf_path"])
                    
                    if available_pdfs:
                        pdf_path = available_pdfs[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„PDF
                        logging.info(f"ğŸ“„ ä½¿ç”¨å¯ç”¨çš„PDFæ–‡ä»¶: {pdf_path}")
                    else:
                        logging.warning("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„PDFæ–‡ä»¶è¿›è¡Œæ‘˜è¦")
                        result = {"error": "æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„PDFæ–‡ä»¶"}
                
                if pdf_path and os.path.exists(pdf_path):
                    result = summarize_pdf.invoke({"path": pdf_path})
                    # PDFæ‘˜è¦ç»“æœå¯ä»¥å­˜å‚¨åˆ°æ‰§è¡Œè®°å½•ä¸­ï¼Œæˆ–è€…æ·»åŠ åˆ°è®ºæ–‡ä¿¡æ¯ä¸­
                    if result and "error" not in result:
                        # å°†æ‘˜è¦æ·»åŠ åˆ°å¯¹åº”çš„è®ºæ–‡ä¿¡æ¯ä¸­
                        for paper in state["arxiv_papers"]:
                            if paper.get("local_pdf_path") == pdf_path:
                                paper["detailed_summary"] = result["summary"]
                                break
            
            # æ¯æ¬¡æ”¶é›†åˆ°æ–°æ•°æ®åï¼Œç«‹å³æ›´æ–°å‚è€ƒæ–‡çŒ®åˆ—è¡¨
            state = self.add_references_from_data(state)
            
            # è®°å½•æ‰§è¡Œç»“æœ
            execution_memory.append({
                "step_id": current_step + 1,
                "action": f"{action_name}({parameters})",
                "description": description,
                "result": str(result)[:200] if result else "æ— ç»“æœ",
                "success": result is not None and (not isinstance(result, list) or len(result) > 0)
            })
            
        except Exception as e:
            logging.error(f"æ‰§è¡Œæ­¥éª¤å¤±è´¥: {e}")
            execution_memory.append({
                "step_id": current_step + 1,
                "action": f"{action_name}({parameters})",
                "description": description,
                "result": f"æ‰§è¡Œå¤±è´¥: {str(e)}",
                "success": False
            })
        
        state["execution_memory"] = execution_memory
        state["current_step"] = current_step + 1
        
        return state
    
    def add_references_from_data(self, state: ProposalState) -> ProposalState:
        """ä»æ”¶é›†çš„æ•°æ®ä¸­æå–å¹¶æ·»åŠ å‚è€ƒæ–‡çŒ®"""
        arxiv_papers = state.get("arxiv_papers", [])
        web_results = state.get("web_search_results", [])
        reference_list = state.get("reference_list", [])
        ref_counter = state.get("ref_counter", 1)
        
        # å¤„ç†ArXivè®ºæ–‡
        for paper in arxiv_papers:
            if "error" not in paper:
                # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨
                paper_title = paper.get('title', 'Unknown')
                existing_ref = next((ref for ref in reference_list if ref.get('title') == paper_title), None)
                
                if not existing_ref:
                    reference_list.append({
                        "id": ref_counter,
                        "type": "ArXiv",
                        "title": paper_title,
                        "authors": paper.get('authors', []),
                        "published": paper.get('published', 'Unknown'),
                        "arxiv_id": paper.get('arxiv_id', ''),
                        "categories": paper.get('categories', []),
                        "summary": paper.get('detailed_summary', paper.get('summary', ''))  # ä¼˜å…ˆä½¿ç”¨è¯¦ç»†æ‘˜è¦
                    })
                    ref_counter += 1
        
        # å¤„ç†ç½‘ç»œæœç´¢ç»“æœå’ŒCrossRefç»“æœ
        for result in web_results:
            if "error" not in result:
                result_title = result.get('title', result.get('url', 'Unknown'))
                existing_ref = next((ref for ref in reference_list if ref.get('title') == result_title), None)
                
                if not existing_ref:
                    # åŒºåˆ†CrossRefå’Œæ™®é€šWebç»“æœ
                    if result.get('doi'):  # CrossRefç»“æœ
                        reference_list.append({
                            "id": ref_counter,
                            "type": "CrossRef",
                            "title": result_title,
                            "authors": result.get('authors', []),
                            "doi": result.get('doi', ''),
                            "journal": result.get('journal', ''),
                            "published": result.get('published', ''),
                            "url": result.get('url', '')
                        })
                    else:  # æ™®é€šWebç»“æœ
                        reference_list.append({
                            "id": ref_counter,
                            "type": "Web",
                            "title": result_title,
                            "url": result.get('url', ''),
                            "content_preview": result.get('content', result.get('snippet', 'No content'))[:200]
                        })
                    ref_counter += 1
        
        state["reference_list"] = reference_list
        state["ref_counter"] = ref_counter
        
        return state
    
    def get_literature_summary_with_refs(self, state: ProposalState) -> str:
        """è·å–å¸¦æœ‰ç»Ÿä¸€ç¼–å·çš„æ–‡çŒ®æ‘˜è¦"""
        reference_list = state.get("reference_list", [])
        
        literature_summary = ""
        
        # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤º
        arxiv_refs = [ref for ref in reference_list if ref.get("type") == "ArXiv"]
        web_refs = [ref for ref in reference_list if ref.get("type") == "Web"]
        
        if arxiv_refs:
            literature_summary += "\n\n**ç›¸å…³ArXivè®ºæ–‡ï¼š**\n"
            for ref in arxiv_refs:
                literature_summary += f"[{ref['id']}] {ref['title']}\n"
                literature_summary += f"   ä½œè€…: {', '.join(ref['authors'])}\n"
                literature_summary += f"   å‘è¡¨æ—¶é—´: {ref['published']}\n"
                literature_summary += f"   æ‘˜è¦: {ref['summary']}\n"
                literature_summary += f"   åˆ†ç±»: {', '.join(ref['categories'])}\n\n"
        
        if web_refs:
            literature_summary += "\n**ç›¸å…³ç½‘ç»œä¿¡æ¯ï¼š**\n"
            for ref in web_refs:
                literature_summary += f"[{ref['id']}] {ref['title']}\n"
                literature_summary += f"   æ¥æº: {ref['url']}\n"
                literature_summary += f"   å†…å®¹æ‘˜è¦: {ref['content_preview']}...\n\n"
        
        return literature_summary
    
    def generate_reference_section(self, state: ProposalState) -> str:
        """ç”Ÿæˆæ ¼å¼åŒ–çš„å‚è€ƒæ–‡çŒ®éƒ¨åˆ†"""
        reference_list = state.get("reference_list", [])
        
        if not reference_list:
            return ""
        
        ref_text = "\n\n## å‚è€ƒæ–‡çŒ®\n\n"
        
        for ref in reference_list:
            if ref["type"] == "ArXiv":
                # ArXivè®ºæ–‡æ ¼å¼
                authors_str = ", ".join(ref["authors"]) if ref["authors"] else "æœªçŸ¥ä½œè€…"
                categories_str = ", ".join(ref["categories"]) if ref["categories"] else ""
                ref_text += f"[{ref['id']}] {authors_str}. {ref['title']}. arXiv:{ref['arxiv_id']} ({ref['published']})"
                if categories_str:
                    ref_text += f". Categories: {categories_str}"
                ref_text += "\n\n"
            elif ref["type"] == "CrossRef":
                # CrossRefè®ºæ–‡æ ¼å¼
                authors_str = ", ".join(ref["authors"]) if ref["authors"] else "æœªçŸ¥ä½œè€…"
                ref_text += f"[{ref['id']}] {authors_str}. {ref['title']}. {ref['journal']} ({ref['published']}). DOI: {ref['doi']}\n\n"
            elif ref["type"] == "Web":
                # ç½‘ç»œèµ„æºæ ¼å¼
                ref_text += f"[{ref['id']}] {ref['title']}. è®¿é—®æ—¶é—´: {datetime.now().strftime('%Y-%m-%d')}. URL: {ref['url']}\n\n"
        
        return ref_text

    def write_introduction_node(self, state: ProposalState) -> ProposalState:
        """ç”Ÿæˆç ”ç©¶è®¡åˆ’ä¹¦çš„å¼•è¨€éƒ¨åˆ†"""
        research_field = state["research_field"]
        research_plan = state["research_plan"]
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡çŒ®æ‘˜è¦
        literature_summary = self.get_literature_summary_with_refs(state)
        
        citation_instruction = """
        **å¼•ç”¨è¦æ±‚ï¼š**
        1. å½“æåŠç›¸å…³ç ”ç©¶æˆ–è§‚ç‚¹æ—¶ï¼Œå¿…é¡»åœ¨å¥æœ«æ·»åŠ å¼•ç”¨æ ‡è®°ï¼Œæ ¼å¼ä¸º [ç¼–å·]
        2. å¼•ç”¨æ ‡è®°å¯¹åº”ä¸Šè¿°æ–‡çŒ®åˆ—è¡¨ä¸­çš„ç¼–å·
        3. ä¾‹å¦‚ï¼šäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›[1]ï¼Œç‰¹åˆ«æ˜¯åœ¨å½±åƒè¯†åˆ«é¢†åŸŸ[2]ã€‚
        4. ä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„å¼•ç”¨ï¼Œåªèƒ½å¼•ç”¨ä¸Šè¿°æä¾›çš„æ–‡çŒ®
        5. å¦‚æœæŸä¸ªè§‚ç‚¹æ¥è‡ªå¤šä¸ªæ–‡çŒ®ï¼Œå¯ä»¥ä½¿ç”¨ [1,2] çš„æ ¼å¼
        """
        
        # ä½¿ç”¨prompts.pyä¸­çš„instruction
        introduction_prompt = f"""
        {proposal_introduction_instruction}
        
        **ç ”ç©¶ä¸»é¢˜ï¼š** {research_field}
        
        **ç ”ç©¶è®¡åˆ’ï¼š**
        {research_plan}
        
        **å·²æ”¶é›†çš„æ–‡çŒ®å’Œä¿¡æ¯ï¼š**
        {literature_summary}
        {citation_instruction}

        è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼ŒæŒ‰ç…§instructionçš„è¦æ±‚ï¼Œä¸º"{research_field}"è¿™ä¸ªç ”ç©¶ä¸»é¢˜æ’°å†™ä¸€ä¸ªå­¦æœ¯è§„èŒƒçš„å¼•è¨€éƒ¨åˆ†ã€‚
        
        è¦æ±‚ï¼š
        1. å¿…é¡»ä½¿ç”¨ä¸­æ–‡æ’°å†™
        2. è‡³å°‘600å­—
        3. ç»“æ„æ¸…æ™°ï¼ŒåŒ…å«ç ”ç©¶ä¸»é¢˜ä»‹ç»ã€é‡è¦æ€§è¯´æ˜ã€ç ”ç©¶ç©ºç™½åˆ°ç ”ç©¶é—®é¢˜çš„æ¨å¯¼
        4. é€‚å½“å¼•ç”¨å·²æ”¶é›†çš„æ–‡çŒ®ï¼Œä½¿ç”¨ä¸Šè¿°ç¼–å·ç³»ç»Ÿ
        5. è¯­è¨€å­¦æœ¯åŒ–ï¼Œé€‚åˆç ”ç©¶è®¡åˆ’ä¹¦
        6. **ä¸è¦åœ¨å¼•è¨€éƒ¨åˆ†åŒ…å«å‚è€ƒæ–‡çŒ®åˆ—è¡¨**ï¼Œåªåœ¨æ­£æ–‡ä¸­ä½¿ç”¨å¼•ç”¨æ ‡è®°
        7. ä½¿ç”¨`# å¼•è¨€`ä½œä¸ºå¼€å¤´
        """
        
        logging.info("ğŸ“ æ­£åœ¨ç”Ÿæˆç ”ç©¶è®¡åˆ’ä¹¦å¼•è¨€éƒ¨åˆ†...")
        response = self.llm.invoke([HumanMessage(content=introduction_prompt)])
        
        # åªä¿å­˜å¼•è¨€æ­£æ–‡ï¼Œä¸åŒ…å«å‚è€ƒæ–‡çŒ®
        state["introduction"] = response.content
        logging.info("âœ… å¼•è¨€éƒ¨åˆ†ç”Ÿæˆå®Œæˆ")
        
        return state

    def write_literature_review_node(self, state: ProposalState) -> ProposalState:
        """ç”Ÿæˆç ”ç©¶è®¡åˆ’ä¹¦çš„æ–‡çŒ®ç»¼è¿°éƒ¨åˆ†"""
        research_field = state["research_field"]
        research_plan = state["research_plan"]
        introduction_content = state.get("introduction", "")
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡çŒ®æ‘˜è¦
        literature_summary = self.get_literature_summary_with_refs(state)
        
        # ç”Ÿæˆå¼•ç”¨æŒ‡å¯¼
        citation_instruction = """
        **å¼•ç”¨è¦æ±‚ï¼š**
        1. å½“æåŠç›¸å…³ç ”ç©¶ã€ç†è®ºæˆ–è§‚ç‚¹æ—¶ï¼Œå¿…é¡»åœ¨å¥æœ«æ·»åŠ å¼•ç”¨æ ‡è®°ï¼Œæ ¼å¼ä¸º [ç¼–å·]
        2. å¼•ç”¨æ ‡è®°å¯¹åº”ä¸Šè¿°æ–‡çŒ®åˆ—è¡¨ä¸­çš„ç¼–å·
        3. ä¾‹å¦‚ï¼šæ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•[1,2]ï¼Œä½†åœ¨å¯è§£é‡Šæ€§æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜[3]ã€‚
        4. ä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„å¼•ç”¨ï¼Œåªèƒ½å¼•ç”¨ä¸Šè¿°æä¾›çš„æ–‡çŒ®
        5. å¦‚æœæŸä¸ªè§‚ç‚¹æ¥è‡ªå¤šä¸ªæ–‡çŒ®ï¼Œå¯ä»¥ä½¿ç”¨ [1,2,3] çš„æ ¼å¼
        6. åœ¨è®ºè¿°ä¸åŒè§‚ç‚¹æˆ–ç ”ç©¶å‘ç°æ—¶ï¼Œè¦æ˜ç¡®æ ‡æ³¨æ¥æº
        7. å¯¹äºé‡è¦çš„ç†è®ºæ¡†æ¶æˆ–æ–¹æ³•è®ºï¼Œå¿…é¡»å¼•ç”¨ç›¸å…³æ–‡çŒ®
        """

        # è¿è´¯æ€§æŒ‡å¯¼
        coherence_instruction = """
        **ä¸å¼•è¨€éƒ¨åˆ†çš„è¿è´¯æ€§è¦æ±‚ï¼š**
        1. ä»”ç»†é˜…è¯»å·²å®Œæˆçš„å¼•è¨€éƒ¨åˆ†ï¼Œç†è§£å…¶ä¸­æå‡ºçš„ç ”ç©¶é—®é¢˜å’Œè¯†åˆ«çš„ç ”ç©¶ç©ºç™½
        2. æ–‡çŒ®ç»¼è¿°åº”è¯¥æ·±åŒ–å’Œæ‹“å±•å¼•è¨€ä¸­ç®€è¦æåŠçš„ç ”ç©¶é¢†åŸŸ
        3. é¿å…é‡å¤å¼•è¨€ä¸­å·²ç»è¯¦ç»†é˜è¿°çš„èƒŒæ™¯ä¿¡æ¯
        4. ä½¿ç”¨æ‰¿æ¥æ€§è¯­è¨€ï¼Œå¦‚"åŸºäºå‰è¿°ç ”ç©¶é—®é¢˜"ã€"é’ˆå¯¹å¼•è¨€ä¸­æå‡ºçš„..."ç­‰
        5. ç¡®ä¿æ–‡çŒ®ç»¼è¿°çš„ç»“è®ºè‡ªç„¶è¿‡æ¸¡åˆ°å¯¹æ‹Ÿè®®ç ”ç©¶çš„å¿…è¦æ€§è®ºè¯
        6. å¯¹å¼•è¨€ä¸­æåŠçš„å…³é”®æ¦‚å¿µå’Œç†è®ºè¿›è¡Œæ›´æ·±å…¥çš„æ–‡çŒ®åˆ†æ
        """
        
        # ä½¿ç”¨prompts.pyä¸­çš„LITERATURE_REVIEW_PROMPT
        literature_review_prompt = f"""
        {LITERATURE_REVIEW_PROMPT.format(research_field=research_field)}
        
        **ç ”ç©¶ä¸»é¢˜ï¼š** {research_field}
        
        **ç ”ç©¶è®¡åˆ’ï¼š**
        {research_plan}
        
        **å·²å®Œæˆçš„å¼•è¨€éƒ¨åˆ†ï¼š**
        {introduction_content}
        
        **å·²æ”¶é›†çš„æ–‡çŒ®å’Œä¿¡æ¯ï¼š**
        {literature_summary}
        
        {citation_instruction}
        
        {coherence_instruction}
        
        è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼ŒæŒ‰ç…§instructionçš„è¦æ±‚ï¼Œä¸º"{research_field}"è¿™ä¸ªç ”ç©¶ä¸»é¢˜æ’°å†™ä¸€ä¸ªå­¦æœ¯è§„èŒƒçš„æ–‡çŒ®ç»¼è¿°éƒ¨åˆ†ã€‚
        
        è¦æ±‚ï¼š
        1. å¿…é¡»ä½¿ç”¨ä¸­æ–‡æ’°å†™
        2. è‡³å°‘800å­—
        3. ç»“æ„æ¸…æ™°ï¼ŒæŒ‰ä¸»é¢˜ç»„ç»‡æ–‡çŒ®ï¼Œä¸è¦é€ç¯‡è®ºæ–‡ä»‹ç»
        4. é‡ç‚¹å…³æ³¨ç ”ç©¶è¶‹åŠ¿ã€ä¸»è¦è§‚ç‚¹ã€ç ”ç©¶æ–¹æ³•å’Œå­˜åœ¨çš„äº‰è®®
        5. è¯†åˆ«ç ”ç©¶ç©ºç™½å’Œä¸è¶³ï¼Œä¸ºåç»­ç ”ç©¶æä¾›ä¾æ®
        6. å¿…é¡»åŒ…å«é€‚å½“çš„æ–‡çŒ®å¼•ç”¨ï¼Œä½¿ç”¨ä¸Šè¿°ç¼–å·ç³»ç»Ÿ
        7. è¯­è¨€å­¦æœ¯åŒ–ï¼Œé€‚åˆç ”ç©¶è®¡åˆ’ä¹¦
        8. é¿å…ç®€å•ç½—åˆ—ï¼Œè¦è¿›è¡Œåˆ†æå’Œç»¼åˆ
        9. **ä¸å¼•è¨€éƒ¨åˆ†ä¿æŒè¿è´¯æ€§**ï¼Œé¿å…é‡å¤å†…å®¹ï¼Œæ·±åŒ–å¼•è¨€ä¸­çš„ç ”ç©¶é—®é¢˜
        10. ä½¿ç”¨æ‰¿æ¥æ€§è¯­è¨€è¿æ¥å¼•è¨€éƒ¨åˆ†çš„å†…å®¹
        """
        
        logging.info("ğŸ“š æ­£åœ¨ç”Ÿæˆç ”ç©¶è®¡åˆ’ä¹¦æ–‡çŒ®ç»¼è¿°éƒ¨åˆ†...")
        response = self.llm.invoke([HumanMessage(content=literature_review_prompt)])
        
        # æ³¨æ„ï¼šæ–‡çŒ®ç»¼è¿°ä¸é‡å¤æ·»åŠ å‚è€ƒæ–‡çŒ®éƒ¨åˆ†ï¼Œå› ä¸ºå¼•è¨€å·²ç»åŒ…å«äº†å®Œæ•´çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨
        state["literature_review"] = response.content
        logging.info("âœ… æ–‡çŒ®ç»¼è¿°éƒ¨åˆ†ç”Ÿæˆå®Œæˆ")
        
        return state

    def write_research_design_node(self, state: ProposalState) -> ProposalState:
        """ç”Ÿæˆç ”ç©¶è®¡åˆ’ä¹¦çš„ç ”ç©¶è®¾è®¡éƒ¨åˆ†"""
        research_field = state["research_field"]
        research_plan = state["research_plan"]
        introduction_content = state.get("introduction", "")
        literature_review_content = state.get("literature_review", "")
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡çŒ®æ‘˜è¦
        literature_summary = self.get_literature_summary_with_refs(state)
        
        # ç”Ÿæˆå¼•ç”¨æŒ‡å¯¼
        citation_instruction = """
        **å¼•ç”¨è¦æ±‚ï¼š**
        1. å½“æåŠç›¸å…³ç ”ç©¶æ–¹æ³•ã€ç†è®ºæ¡†æ¶æˆ–æŠ€æœ¯æ—¶ï¼Œå¿…é¡»åœ¨å¥æœ«æ·»åŠ å¼•ç”¨æ ‡è®°ï¼Œæ ¼å¼ä¸º [ç¼–å·]
        2. å¼•ç”¨æ ‡è®°å¯¹åº”æ–‡çŒ®åˆ—è¡¨ä¸­çš„ç¼–å·
        3. ä¾‹å¦‚ï¼šæœ¬ç ”ç©¶å°†é‡‡ç”¨æ··åˆæ–¹æ³•ç ”ç©¶è®¾è®¡[5]ï¼Œç»“åˆå®šé‡åˆ†æå’Œå®šæ€§è®¿è°ˆ[8,12]ã€‚
        4. ä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„å¼•ç”¨ï¼Œåªèƒ½å¼•ç”¨å·²æä¾›çš„æ–‡çŒ®
        5. åœ¨æè¿°æ–¹æ³•è®ºä¾æ®æ—¶è¦æ˜ç¡®æ ‡æ³¨æ¥æº
        6. å¯¹äºé‡è¦çš„åˆ†æå·¥å…·å’ŒæŠ€æœ¯æ¡†æ¶ï¼Œå¿…é¡»å¼•ç”¨ç›¸å…³æ–‡çŒ®
        """

        # è¿è´¯æ€§æŒ‡å¯¼
        coherence_instruction = """
        **ä¸å‰æ–‡çš„è¿è´¯æ€§è¦æ±‚ï¼š**
        1. ä»”ç»†åˆ†æå¼•è¨€éƒ¨åˆ†æå‡ºçš„å…·ä½“ç ”ç©¶é—®é¢˜ï¼Œç¡®ä¿ç ”ç©¶è®¾è®¡èƒ½å¤Ÿå›ç­”è¿™äº›é—®é¢˜
        2. åŸºäºæ–‡çŒ®ç»¼è¿°ä¸­è¯†åˆ«çš„æ–¹æ³•è®ºè¶‹åŠ¿å’Œç ”ç©¶ç©ºç™½ï¼Œé€‰æ‹©åˆé€‚çš„ç ”ç©¶æ–¹æ³•
        3. æ‰¿æ¥æ–‡çŒ®ç»¼è¿°ä¸­æåˆ°çš„ç†è®ºæ¡†æ¶å’Œåˆ†ææ–¹æ³•ï¼Œè¯´æ˜å¦‚ä½•åœ¨æœ¬ç ”ç©¶ä¸­åº”ç”¨æˆ–æ”¹è¿›
        4. ä½¿ç”¨æ‰¿æ¥æ€§è¯­è¨€ï¼Œå¦‚"åŸºäºå‰è¿°æ–‡çŒ®åˆ†æ"ã€"é’ˆå¯¹å¼•è¨€ä¸­æå‡ºçš„ç ”ç©¶é—®é¢˜"ã€"å€Ÿé‰´æ–‡çŒ®ç»¼è¿°ä¸­çš„..."ç­‰
        5. ç¡®ä¿ç ”ç©¶è®¾è®¡çš„æ¯ä¸ªç»„æˆéƒ¨åˆ†éƒ½ä¸å‰æ–‡å»ºç«‹çš„ç ”ç©¶èƒŒæ™¯å’Œç†è®ºåŸºç¡€ç›¸å‘¼åº”
        6. æ˜ç¡®è¯´æ˜ä¸ºä»€ä¹ˆé€‰æ‹©çš„æ–¹æ³•é€‚åˆè§£å†³å¼•è¨€ä¸­æå‡ºçš„ç ”ç©¶é—®é¢˜
        """
        
        # ä½¿ç”¨prompts.pyä¸­çš„PROJECT_DESIGN_PROMPT
        research_design_prompt = f"""
        {PROJECT_DESIGN_PROMPT.format(research_field=research_field)}
        
        **ç ”ç©¶ä¸»é¢˜ï¼š** {research_field}
        
        **ç ”ç©¶è®¡åˆ’æ¦‚è¦ï¼š**
        {research_plan}
        
        **å·²å®Œæˆçš„å¼•è¨€éƒ¨åˆ†ï¼š**
        {introduction_content}
        
        **å·²å®Œæˆçš„æ–‡çŒ®ç»¼è¿°éƒ¨åˆ†ï¼š**
        {literature_review_content}
        
        **å·²æ”¶é›†çš„æ–‡çŒ®å’Œä¿¡æ¯ï¼ˆç”¨äºå¯èƒ½çš„å¼•ç”¨ï¼‰ï¼š**
        {literature_summary}
        
        {citation_instruction}
        
        {coherence_instruction}
        
        è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼ŒæŒ‰ç…§instructionçš„è¦æ±‚ï¼Œä¸ºâ€œ{research_field}â€è¿™ä¸ªç ”ç©¶ä¸»é¢˜æ’°å†™ä¸€ä¸ªå­¦æœ¯è§„èŒƒçš„ç ”ç©¶è®¾è®¡éƒ¨åˆ†ã€‚
        é‡ç‚¹å…³æ³¨ç ”ç©¶æ•°æ®ã€æ–¹æ³•ã€å·¥ä½œæµç¨‹å’Œå±€é™æ€§ã€‚
        å¿…é¡»**ä½¿ç”¨ä¸­æ–‡æ’°å†™**
        **ä¸è¦åŒ…å«æ—¶é—´å®‰æ’æˆ–é¢„æœŸæˆæœæ€»ç»“ï¼Œè¿™äº›å°†åœ¨ç»“è®ºéƒ¨åˆ†ç»Ÿä¸€é˜è¿°ã€‚**
        """
        
        logging.info("ğŸ”¬ æ­£åœ¨ç”Ÿæˆç ”ç©¶è®¡åˆ’ä¹¦ç ”ç©¶è®¾è®¡éƒ¨åˆ†...")
        response = self.llm.invoke([HumanMessage(content=research_design_prompt)])
        
        state["research_design"] = response.content
        logging.info("âœ… ç ”ç©¶è®¾è®¡éƒ¨åˆ†ç”Ÿæˆå®Œæˆ")
        
        return state


    def write_conclusion_node(self, state: ProposalState) -> ProposalState:
        """ç”Ÿæˆç ”ç©¶è®¡åˆ’ä¹¦çš„ç»“è®ºéƒ¨åˆ†"""
        research_field = state["research_field"]
        introduction_content = state.get("introduction", "")
        literature_review_content = state.get("literature_review", "")
        research_design_content = state.get("research_design", "")
        
        conclusion_prompt_text = f"""
        {CONCLUSION_PROMPT.format(research_field=research_field)}

        **ç ”ç©¶ä¸»é¢˜ï¼š** {research_field}

        **å·²å®Œæˆçš„å¼•è¨€éƒ¨åˆ†æ‘˜è¦ï¼ˆç”¨äºå›é¡¾ç ”ç©¶é—®é¢˜å’ŒèƒŒæ™¯ï¼‰ï¼š**
        {introduction_content[:1000]}... 

        **å·²å®Œæˆçš„æ–‡çŒ®ç»¼è¿°éƒ¨åˆ†æ‘˜è¦ï¼ˆç”¨äºå›é¡¾ç†è®ºæ¡†æ¶ï¼‰ï¼š**
        {literature_review_content[:1000]}...

        **å·²å®Œæˆçš„ç ”ç©¶è®¾è®¡éƒ¨åˆ†æ‘˜è¦ï¼ˆç”¨äºå›é¡¾æ–¹æ³•å’Œæµç¨‹ï¼‰ï¼š**
        {research_design_content[:1000]}...

        è¯·åŸºäºä»¥ä¸Šæä¾›çš„å¼•è¨€ã€æ–‡çŒ®ç»¼è¿°å’Œç ”ç©¶è®¾è®¡å†…å®¹ï¼Œæ’°å†™ä¸€ä¸ªè¿è´¯çš„ç»“è®ºéƒ¨åˆ†ã€‚
        ç»“è®ºåº”åŒ…å«æ—¶é—´è½´ã€é¢„æœŸæˆæœå’Œæœ€ç»ˆæ€»ç»“ã€‚
        ç¡®ä¿ç»“è®ºä¸å‰é¢ç« èŠ‚æå‡ºçš„ç ”ç©¶é—®é¢˜ã€æ–¹æ³•è®ºå’Œç›®æ ‡ä¿æŒä¸€è‡´ã€‚
        å¿…é¡»ä½¿ç”¨**ä¸­æ–‡**æ’°å†™
        """
        
        logging.info("ğŸ“œ æ­£åœ¨ç”Ÿæˆç ”ç©¶è®¡åˆ’ä¹¦ç»“è®ºéƒ¨åˆ†...")
        response = self.llm.invoke([HumanMessage(content=conclusion_prompt_text)])
        
        state["conclusion"] = response.content
        logging.info("âœ… ç»“è®ºéƒ¨åˆ†ç”Ÿæˆå®Œæˆ")
        
        return state


    def generate_final_references_node(self, state: ProposalState) -> ProposalState:
        """ç”Ÿæˆæœ€ç»ˆçš„å‚è€ƒæ–‡çŒ®éƒ¨åˆ†"""
        reference_section = self.generate_reference_section(state)
        
        # å°†å‚è€ƒæ–‡çŒ®ä½œä¸ºç‹¬ç«‹éƒ¨åˆ†ä¿å­˜
        state["final_references"] = reference_section
        logging.info("âœ… å‚è€ƒæ–‡çŒ®éƒ¨åˆ†ç”Ÿæˆå®Œæˆ")
        
        return state

    def generate_final_report_node(self, state: ProposalState) -> ProposalState:
        """ç”Ÿæˆæœ€ç»ˆçš„Markdownç ”ç©¶è®¡åˆ’ä¹¦æŠ¥å‘Š"""
        logging.info("ğŸ“„ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆçš„ç ”ç©¶è®¡åˆ’ä¹¦MarkdownæŠ¥å‘Š...")
        
        research_field = state.get("research_field", "æœªçŸ¥é¢†åŸŸ")
        introduction = state.get("introduction", "æ— å¼•è¨€å†…å®¹")
        literature_review = state.get("literature_review", "æ— æ–‡çŒ®ç»¼è¿°å†…å®¹")
        research_design = state.get("research_design", "æ— ç ”ç©¶è®¾è®¡å†…å®¹")
        conclusion = state.get("conclusion", "æ— ç»“è®ºå†…å®¹")
        final_references = state.get("final_references", "æ— å‚è€ƒæ–‡çŒ®")
        
        research_plan = state.get("research_plan", "æ— åˆå§‹ç ”ç©¶è®¡åˆ’")
        execution_memory = state.get("execution_memory", [])
        
        # åˆ›å»ºoutputæ–‡ä»¶å¤¹
        output_dir = "./output"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        # æ–‡ä»¶ååŒ…å«æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_research_field = "".join(c for c in research_field if c.isalnum() or c in (' ', '-', '_')).rstrip().replace(' ', '_')[:30]
        report_filename = f"Research_Proposal_{safe_research_field}_{timestamp}.md"
        report_filepath = os.path.join(output_dir, report_filename)
        
        # æ„å»ºMarkdownå†…å®¹
        report_content = f"# ç ”ç©¶è®¡åˆ’ä¹¦ï¼š{research_field}\n\n"
        
        # report_content += "## 1. å¼•è¨€\n\n"
        report_content += f"{introduction}\n\n"
        
        # report_content += "## 2. æ–‡çŒ®ç»¼è¿°\n\n"
        report_content += f"{literature_review}\n\n"
        
        # report_content += "## 3. ç ”ç©¶è®¾è®¡ä¸æ–¹æ³•\n\n"
        report_content += f"{research_design}\n\n"
        
        # report_content += "## 4. ç»“è®ºä¸å±•æœ›\n\n" # ç»“è®ºéƒ¨åˆ†å·²åŒ…å«æ—¶é—´è½´å’Œé¢„æœŸæˆæœ
        report_content += f"{conclusion}\n\n"
        
        report_content += f"{final_references}\n\n" # å‚è€ƒæ–‡çŒ®éƒ¨åˆ†è‡ªå¸¦ "## å‚è€ƒæ–‡çŒ®" æ ‡é¢˜
        
        report_content += "---\n"
        report_content += "## é™„å½•ï¼šè¿‡ç¨‹èµ„æ–™\n\n"
        
        report_content += "### A.1 åˆå§‹ç ”ç©¶è®¡åˆ’\n\n"
        report_content += "```markdown\n"
        report_content += f"{research_plan}\n"
        report_content += "```\n\n"
        
        report_content += "### A.2 æ‰§è¡Œæ­¥éª¤è®°å½•\n\n"
        if execution_memory:
            for i, step_memory in enumerate(execution_memory):
                action = step_memory.get("action", "æœªçŸ¥åŠ¨ä½œ")
                desc = step_memory.get("description", "æ— æè¿°")
                res = step_memory.get("result", "æ— ç»“æœ")
                success_status = "æˆåŠŸ" if step_memory.get("success") else "å¤±è´¥"
                report_content += f"**æ­¥éª¤ {i+1}: {desc}** ({action})\n"
                report_content += f"- çŠ¶æ€: {success_status}\n"
                report_content += f"- ç»“æœæ‘˜è¦: {str(res)[:150]}...\n\n"
        else:
            report_content += "æ— æ‰§è¡Œè®°å½•ã€‚\n\n"
            
        report_content += "### A.3 æ”¶é›†çš„æ–‡çŒ®ä¸ä¿¡æ¯æ‘˜è¦\n\n"
        report_content += self.get_literature_summary_with_refs(state) + "\n\n"

        try:
            with open(report_filepath, 'w', encoding='utf-8') as f:
                f.write(report_content)
            logging.info(f"âœ… æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filepath}")
            state["final_report_markdown"] = report_content
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜æœ€ç»ˆæŠ¥å‘Šå¤±è´¥: {e}")
            state["final_report_markdown"] = "æŠ¥å‘Šç”Ÿæˆå¤±è´¥"
            
        return state

    def should_continue(self, state: ProposalState) -> str:
        """å†³å®šæ˜¯å¦ç»§ç»­æ‰§è¡Œæˆ–è¿›å…¥å†™ä½œé˜¶æ®µ"""
        current_step = state.get("current_step", 0)
        execution_plan = state.get("execution_plan", [])
        execution_memory = state.get("execution_memory", [])
        max_iterations = state.get("max_iterations", 10)
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
        if len(execution_memory) >= max_iterations:
            logging.info("è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè¿›å…¥å†™ä½œé˜¶æ®µ")
            return "write_introduction"
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ­¥éª¤è¦æ‰§è¡Œ
        if current_step < len(execution_plan):
            return "execute_step"
        
        # æ£€æŸ¥æ˜¯å¦æ”¶é›†åˆ°è¶³å¤Ÿçš„ä¿¡æ¯
        arxiv_papers = state.get("arxiv_papers", [])
        web_results = state.get("web_search_results", [])
        
        logging.info(f"å½“å‰æ”¶é›†æƒ…å†µ: {len(arxiv_papers)} ç¯‡è®ºæ–‡, {len(web_results)} æ¡ç½‘ç»œç»“æœ")
        
        # å¦‚æœå·²ç»æ”¶é›†åˆ°è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œè¿›å…¥å†™ä½œé˜¶æ®µ
        if len(arxiv_papers) >= 3 or len(web_results) >= 3:
            logging.info("å·²æ”¶é›†åˆ°è¶³å¤Ÿä¿¡æ¯ï¼Œè¿›å…¥å†™ä½œé˜¶æ®µ")
            return "write_introduction"
        
        # æ£€æŸ¥æœ€è¿‘çš„æ‰§è¡Œç»“æœ
        recent_results = execution_memory[-3:] if len(execution_memory) >= 3 else execution_memory
        successful_results = [r for r in recent_results if r.get("success", False)]
        
        # å¦‚æœæœ€è¿‘çš„ç»“æœéƒ½ä¸æˆåŠŸï¼Œé‡æ–°è§„åˆ’
        if len(successful_results) < len(recent_results) * 0.3:
            logging.info("æœ€è¿‘æ‰§è¡Œç»“æœä¸ç†æƒ³ï¼Œé‡æ–°è§„åˆ’...")
            state["current_step"] = 0
            return "plan_analysis"
        
        # å¦‚æœæ‰§è¡Œäº†ä¸€è½®ä½†ä¿¡æ¯ä¸è¶³ï¼Œç»§ç»­è§„åˆ’
        if len(arxiv_papers) < 3 and len(web_results) < 3:
            logging.info("ä¿¡æ¯æ”¶é›†ä¸è¶³ï¼Œç»§ç»­è§„åˆ’...")
            state["current_step"] = 0
            return "plan_analysis"
        
        # é»˜è®¤è¿›å…¥å†™ä½œé˜¶æ®µ
        return "write_introduction"
    
    
    
    def _build_workflow(self) -> StateGraph:
        """æ„å»ºå·¥ä½œæµå›¾"""
        workflow = StateGraph(ProposalState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("create_master_plan", self.create_master_plan_node)
        workflow.add_node("plan_analysis", self.plan_analysis_node)
        workflow.add_node("execute_step", self.execute_step_node)
        workflow.add_node("write_introduction", self.write_introduction_node)
        workflow.add_node("write_literature_review", self.write_literature_review_node)
        workflow.add_node("write_research_design", self.write_research_design_node)
        workflow.add_node("write_conclusion", self.write_conclusion_node) 
        workflow.add_node("generate_final_references", self.generate_final_references_node)
        workflow.add_node("generate_final_report", self.generate_final_report_node) # æ–°å¢æœ€ç»ˆæŠ¥å‘ŠèŠ‚ç‚¹
        
        # å®šä¹‰æµç¨‹
        workflow.set_entry_point("create_master_plan")
        workflow.add_edge("create_master_plan", "plan_analysis")
        
        # æ¡ä»¶è¾¹ï¼šæ ¹æ®æ‰§è¡Œæƒ…å†µå†³å®šä¸‹ä¸€æ­¥
        workflow.add_conditional_edges(
            "plan_analysis",
            lambda state: "execute_step",  # ç”Ÿæˆè®¡åˆ’åæ‰§è¡Œæ­¥éª¤
            {"execute_step": "execute_step"}
        )
        
        workflow.add_conditional_edges(
            "execute_step",
            self.should_continue,  # æ ¹æ®æ‰§è¡Œç»“æœå†³å®šä¸‹ä¸€æ­¥
            {
                "execute_step": "execute_step",  # ç»§ç»­æ‰§è¡Œä¸‹ä¸€æ­¥
                "plan_analysis": "plan_analysis",  # é‡æ–°è§„åˆ’
                "write_introduction": "write_introduction" 
            }
        )
        
        workflow.add_edge("write_introduction", "write_literature_review")
        workflow.add_edge("write_literature_review", "write_research_design")
        workflow.add_edge("write_research_design", "write_conclusion") 
        workflow.add_edge("write_conclusion", "generate_final_references") 
        workflow.add_edge("generate_final_references", "generate_final_report") # å‚è€ƒæ–‡çŒ®ååˆ°æœ€ç»ˆæŠ¥å‘Š
        workflow.add_edge("generate_final_report", END) # æœ€ç»ˆæŠ¥å‘Šåç»“æŸ
        
        return workflow.compile()
    
    def generate_proposal(self, research_field: str) -> Dict[str, Any]:
        """ç”Ÿæˆç ”ç©¶è®¡åˆ’ä¹¦"""
        initial_state = ProposalState(
            research_field=research_field,
            query="",
            arxiv_papers=[],
            web_search_results=[],
            background="",
            objectives="",
            methodology="",
            timeline="",
            expected_outcomes="",
            final_proposal="",
            messages=[],
            research_plan="",
            available_tools=[],
            execution_plan=[],
            execution_memory=[],
            current_step=0,
            max_iterations=10,
            introduction="",
            literature_review="",
            research_design="",
            timeline_plan="",
            expected_results="",
            reference_list=[],  # åˆå§‹åŒ–ç»Ÿä¸€å‚è€ƒæ–‡çŒ®åˆ—è¡¨
            ref_counter=1,      # åˆå§‹åŒ–å‚è€ƒæ–‡çŒ®è®¡æ•°å™¨
            final_references="", 
            conclusion="",       
            final_report_markdown="" # åˆå§‹åŒ–æœ€ç»ˆæŠ¥å‘Šå­—æ®µ
        )
        
        logging.info(f"ğŸš€ å¼€å§‹å¤„ç†ç ”ç©¶é—®é¢˜: '{research_field}'")
        result = self.workflow.invoke(initial_state)
        return result


"""
TODO: å·²å®Œæˆç®€å•çš„æœç´¢åŠŸèƒ½ç­‰å†…å®¹
ä¸‹ä¸€æ­¥ï¼šç”ŸæˆæŠ¥å‘Šç›¸å…³
"""

if __name__ == "__main__":
    # æµ‹è¯•PDFæ‘˜è¦åŠŸèƒ½
    # pdf_result = summarize_pdf.invoke({"path": "./Papers/test.pdf"})
    # print("PDFæ‘˜è¦æµ‹è¯•:", pdf_result)
    
    agent = ProposalAgent()
    research_question = "RAGä¸æ¨ç†æŠ€æœ¯çš„ç»“åˆ"
    result = agent.generate_proposal(research_question)
    print("\n" + "="*60)
    # print("è®¡åˆ’:")
    # print(result["research_plan"])
    print("\n" + "="*60)
    print(f"æ‰§è¡Œå†å²: {len(result['execution_memory'])} ä¸ªæ­¥éª¤")
    for memory in result["execution_memory"]:
        print(f"- {memory['description']}: {'æˆåŠŸ' if memory['success'] else 'å¤±è´¥'}")
    print("\n" + "="*60)
    print(f"æ”¶é›†åˆ°çš„è®ºæ–‡: {len(result['arxiv_papers'])} ç¯‡")
    print(f"ç½‘ç»œæœç´¢ç»“æœ: {len(result['web_search_results'])} æ¡")
    print(f"ç»Ÿä¸€å‚è€ƒæ–‡çŒ®: {len(result['reference_list'])} æ¡")
    print("\n" + "="*60)
    # print("å¼•è¨€éƒ¨åˆ†:")
    # print(result["introduction"])
    # print("\n" + "="*60)
    # print("æ–‡çŒ®ç»¼è¿°éƒ¨åˆ†:")
    # print(result["literature_review"])
    # print("\n" + "="*60)
    # print("ç ”ç©¶è®¾è®¡éƒ¨åˆ†:")
    # print(result["research_design"])
    # print("\n" + "="*60)
    # print("ç»“è®ºéƒ¨åˆ†:") 
    # print(result["conclusion"])
    # print("\n" + "="*60)
    # print("å‚è€ƒæ–‡çŒ®éƒ¨åˆ†:")
    # print(result["final_references"])

    # è¾“å‡ºæœ€ç»ˆæŠ¥å‘Šçš„ä¿å­˜è·¯å¾„æˆ–å†…å®¹
    if result.get("final_report_markdown") and result["final_report_markdown"] != "æŠ¥å‘Šç”Ÿæˆå¤±è´¥":
        # æŸ¥æ‰¾æŠ¥å‘Šæ–‡ä»¶åï¼Œå› ä¸ºè·¯å¾„æ˜¯åœ¨å‡½æ•°å†…éƒ¨ç”Ÿæˆçš„
        output_dir = "./output"
        if os.path.exists(output_dir):
            files = sorted(os.listdir(output_dir), key=lambda x: os.path.getmtime(os.path.join(output_dir, x)), reverse=True)
            if files:
                latest_report = os.path.join(output_dir, files[0])
                print(f"âœ… æœ€ç»ˆç ”ç©¶è®¡åˆ’ä¹¦å·²ç”Ÿæˆå¹¶ä¿å­˜åˆ°: {latest_report}")
            else:
                print("âœ… æœ€ç»ˆç ”ç©¶è®¡åˆ’ä¹¦å†…å®¹å·²ç”Ÿæˆï¼Œä½†æœªæ‰¾åˆ°å…·ä½“æ–‡ä»¶è·¯å¾„ã€‚")
        else:
             print("âœ… æœ€ç»ˆç ”ç©¶è®¡åˆ’ä¹¦å†…å®¹å·²ç”Ÿæˆã€‚")
        # print("\næŠ¥å‘Šå†…å®¹é¢„è§ˆ:\n", result["final_report_markdown"][:1000] + "...") # å¯ä»¥é€‰æ‹©æ€§æ‰“å°éƒ¨åˆ†å†…å®¹
    else:
        print("âŒ æœªèƒ½ç”Ÿæˆæœ€ç»ˆç ”ç©¶è®¡åˆ’ä¹¦æŠ¥å‘Šã€‚")