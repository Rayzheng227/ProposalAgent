"""
è¿‡ç¨‹ä¸­æ¶‰åŠåˆ°çš„ä¸€äº›å·¥å…·ï¼Œå·¥å…·ç›¸å…³é…ç½®è§:tools.json
"""
from concurrent.futures import ThreadPoolExecutor, Future, TimeoutError as FuturesTimeoutError
import arxiv
from langchain_core.tools import tool
import logging
import os
from dotenv import load_dotenv
from typing import List, Dict 
from langchain_community.tools import TavilySearchResults
from crossref.restful import Works
from langchain_core.messages import HumanMessage, SystemMessage
import fitz
from langchain_openai import ChatOpenAI
import backend.src.agent.rag as rag
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError

load_dotenv()
TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY")
DASHSCOPE_API_KEY = os.environ.get("DASHSCOPE_API_KEY")
base_url = os.environ.get("DASHSCOPE_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")

@tool
def search_arxiv_papers_tool(query: str, max_results: int = 10, Download: bool = True) -> List[Dict]:
    """æœç´¢å¹¶ä¸‹è½½ArXivè®ºæ–‡çš„å·¥å…·

    Args:
        query: æœç´¢å…³é”®è¯
        max_results: æœ€å¤§ç»“æœæ•°é‡ï¼Œé»˜è®¤5ç¯‡
        Download: æ˜¯å¦ä¸‹è½½PDFæ–‡ä»¶

    Returns:
        åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        ä»¥åŠå­˜å‚¨åœ¨Papersç›®å½•ä¸‹çš„å‚è€ƒæ–‡çŒ®
    """
    logging.info(f"åœ¨arxivä¸Šæœç´¢é¢†åŸŸä¸º:{query}")

    try:
        content = rag.generate_search_queries(query)
        queries = [line.strip() for line in content.split('\n') if line.strip()]
        logging.info(f"åœ¨arxivä¸Šæœç´¢å…³é”®è¯ä¸º:{queries}")
        papers = []
        seen_ids = set()
        
        # æ·»åŠ SSLå’Œè¿æ¥é…ç½®
        import ssl
        import urllib3
        
        # ç¦ç”¨SSLè­¦å‘Š
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        for q in queries:
            retry_count = 0
            max_retries = 3
            
            while retry_count < max_retries:
                try:
                    # åˆ›å»ºå®¢æˆ·ç«¯æ—¶æ·»åŠ å»¶è¿Ÿå’Œé‡è¯•æœºåˆ¶
                    client = arxiv.Client(
                        page_size=min(10, max(2, max_results // len(queries))),
                        delay_seconds=3,  # å¢åŠ å»¶è¿Ÿ
                        num_retries=2
                    )
                    
                    search = arxiv.Search(
                        query=q,
                        max_results=max(2, max_results // len(queries)),
                        sort_by=arxiv.SortCriterion.SubmittedDate
                    )

                    papers_dir = "./Papers"
                    if not os.path.exists(papers_dir):
                        os.makedirs(papers_dir)

                    # æ·»åŠ è¶…æ—¶æ§åˆ¶
                    import time
                    search_start_time = time.time()
                    timeout_seconds = 30  # 30ç§’è¶…æ—¶
                    
                    for paper in client.results(search):
                        # æ£€æŸ¥è¶…æ—¶
                        if time.time() - search_start_time > timeout_seconds:
                            logging.warning(f"ArXivæœç´¢è¶…æ—¶ï¼Œåœæ­¢å½“å‰æŸ¥è¯¢: {q}")
                            break
                            
                        if paper.entry_id in seen_ids:
                            continue
                            
                        paper_info = {
                            "title": paper.title,
                            "authors": [author.name for author in paper.authors],
                            "summary": paper.summary[:300] + "...",  # æˆªæ–­æ‘˜è¦
                            "published": paper.published.strftime("%Y-%m-%d"),
                            "pdf_url": paper.pdf_url,
                            "categories": paper.categories,
                            "arxiv_id": paper.entry_id.split('/')[-1]
                        }

                        if Download:
                            try:
                                # ä¸‹è½½PDF - æ”¹è¿›æ–‡ä»¶åå¤„ç†å’Œé”™è¯¯å¤„ç†
                                logging.info(f"æ­£åœ¨ä¸‹è½½è®ºæ–‡ï¼š{paper.title[:50]}...")

                                # æ›´å®‰å…¨çš„æ–‡ä»¶åå¤„ç†
                                import re
                                safe_title = re.sub(r'[^\w\s-]', '', paper.title)  # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
                                safe_title = re.sub(r'[-\s]+', '-', safe_title)    # æ›¿æ¢ç©ºæ ¼å’Œå¤šä¸ªè¿å­—ç¬¦
                                safe_title = safe_title.strip('-')[:40]             # é™åˆ¶é•¿åº¦å¹¶ç§»é™¤é¦–å°¾è¿å­—ç¬¦

                                if not safe_title:  # å¦‚æœæ ‡é¢˜å¤„ç†åä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
                                    safe_title = "paper"

                                filename = f"{paper_info['arxiv_id']}_{safe_title}.pdf"
                                full_path = os.path.join(papers_dir, filename)

                                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                                if os.path.exists(full_path):
                                    logging.info(f"è®ºæ–‡å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {filename}")
                                    paper_info["local_pdf_path"] = full_path
                                else:
                                    # ä½¿ç”¨æ›´ç¨³å®šçš„ä¸‹è½½æ–¹æ³•
                                    time.sleep(5)  # å¢åŠ ä¸‹è½½é—´éš”æ—¶é—´ï¼Œä¾‹å¦‚5ç§’ï¼Œä»¥å‡å°‘æœåŠ¡å™¨å‹åŠ›

                                    paper.download_pdf(dirpath=papers_dir, filename=filename)

                                    # éªŒè¯ä¸‹è½½æ˜¯å¦æˆåŠŸ
                                    if os.path.exists(full_path) and os.path.getsize(full_path) > 0:
                                        paper_info["local_pdf_path"] = full_path
                                        logging.info(f"âœ… æˆåŠŸä¸‹è½½: {filename}")
                                    else:
                                        paper_info["local_pdf_path"] = None
                                        logging.warning(f"âŒ ä¸‹è½½å¤±è´¥æˆ–æ–‡ä»¶ä¸ºç©º: {filename}")

                            except Exception as e:
                                paper_info["local_pdf_path"] = None
                                logging.warning(f"âŒ ä¸‹è½½è®ºæ–‡å¤±è´¥: {paper.title[:50]}... - é”™è¯¯: {str(e)}")

                        seen_ids.add(paper.entry_id)
                        papers.append(paper_info)

                        # é™åˆ¶å¤„ç†æ•°é‡ï¼Œé¿å…è¿‡å¤šè¯·æ±‚
                        if len(papers) >= max_results:
                            break
                    
                    # å¦‚æœæˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    break
                    
                except Exception as search_error:
                    retry_count += 1
                    error_str = str(search_error).lower()
                    
                    if retry_count < max_retries:
                        wait_time = retry_count * 5  # é€’å¢ç­‰å¾…æ—¶é—´
                        logging.warning(f"ArXivæœç´¢å¤±è´¥ (å°è¯• {retry_count}/{max_retries}): {str(search_error)}")
                        logging.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                    else:
                        logging.error(f"ArXivæœç´¢æœ€ç»ˆå¤±è´¥: {str(search_error)}")
                        
                        # æä¾›è¯¦ç»†çš„é”™è¯¯è¯Šæ–­
                        if "ssl" in error_str or "eof" in error_str:
                            logging.error("SSLè¿æ¥é”™è¯¯ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–ArXivæœåŠ¡å™¨é—®é¢˜")
                        elif "timeout" in error_str:
                            logging.error("è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
                        elif "max retries" in error_str:
                            logging.error("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒArXivæœåŠ¡å¯èƒ½æš‚æ—¶ä¸å¯ç”¨")

        logging.info(f"âœ… ArXivæœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        successful_downloads = len([p for p in papers if p.get("local_pdf_path")])
        logging.info(f"ğŸ“„ æˆåŠŸä¸‹è½½ {successful_downloads} ä¸ªPDFæ–‡ä»¶")

        return papers

    except Exception as e:
        logging.error(f"âŒ ArXivæœç´¢å¤±è´¥: {str(e)}")
        return [{"error": f"ArXivæœç´¢å¤±è´¥: {str(e)}"}]


@tool
def search_web_content_tool(query: str) -> List[Dict]:
    """ä½¿ç”¨Tavilyæœç´¢ç½‘ç»œå†…å®¹çš„å·¥å…·

    Args:
        query: æœç´¢æŸ¥è¯¢

    Returns:
        æœç´¢ç»“æœåˆ—è¡¨
    """
    logging.info(f"æ­£åœ¨ç½‘ç»œæœç´¢é¢†åŸŸ:{query}")
    queries = rag.generate_search_queries(query)
    logging.info(f"æ­£åœ¨ç½‘ç»œæœç´¢å…³é”®è¯:{queries}")

    try:
        os.environ["TAVILY_API_KEY"] = TAVILY_API_KEY
        tavily_tool = TavilySearchResults(
            max_results=5,
            search_depth="advanced",
            include_answer=True,
            include_raw_content=True
        )

        results = tavily_tool.invoke({"query": queries})
        return results

    except Exception as e:
        return [{"error": f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}"}]


@tool
def search_crossref_papers_tool(query: str, max_results: int = 5) -> List[Dict]:
    """ä½¿ç”¨ CrossRef æœç´¢è®ºæ–‡å…ƒæ•°æ®çš„å·¥å…·

    Args:
        query: å…³é”®è¯æˆ–ä¸»é¢˜
        max_results: è¿”å›ç»“æœæ•°é‡ä¸Šé™ï¼ˆé»˜è®¤5ï¼‰

    Returns:
        åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
    """
    logging.info(f"åœ¨crossrefä¸Šæœç´¢é¢†åŸŸ:{query}")
    queries = rag.generate_search_queries(query)
    logging.info(f"åœ¨crossrefä¸Šæœç´¢é¢†åŸŸ:{queries}")

    try:
        works = Works()
        search = works.query(queries).sort('relevance')

        results = []
        for i, item in enumerate(search):
            if i >= max_results:
                break

            paper_info = {
                "title": item.get("title", ["No title"])[0],
                "authors": [
                    f"{author.get('given', '')} {author.get('family', '')}".strip()
                    for author in item.get("author", [])
                ],
                "doi": item.get("DOI", "N/A"),
                "published": "-".join(str(d) for d in item.get("issued", {}).get("date-parts", [[None]])[0]),
                "publisher": item.get("publisher", "N/A"),
                "journal": item.get("container-title", ["N/A"])[0],
                "url": item.get("URL", "N/A")
            }

            results.append(paper_info)

        return results

    except Exception as e:
        return [{"error": f"CrossRef æœç´¢å¤±è´¥: {str(e)}"}]


@tool
def summarize_pdf(path: str, max_chars: int = 10000) -> Dict:
    """æ€»ç»“PDFæ–‡ä»¶å†…å®¹çš„å·¥å…·
    
    Args:
        path: PDFæ–‡ä»¶è·¯å¾„
        max_chars: æœ€å¤§å­—ç¬¦æ•°é™åˆ¶ï¼Œé»˜è®¤10000
        
    Returns:
        åŒ…å«æ‘˜è¦å’Œæºæ–‡æœ¬ç‰‡æ®µçš„å­—å…¸ã€‚å¦‚æœæ‘˜è¦ç”Ÿæˆè¶…æ—¶æˆ–å¤±è´¥ï¼Œæ‘˜è¦å†…å®¹å°†ä¸ºç©ºå­—ç¬¦ä¸²ã€‚
    """
    logging.info(f"è°ƒç”¨å·¥å…·ï¼šsummarize_pdf:{path}")
    
    full_text = ""
    source_excerpt = ""
    total_length = 0

    try:
        # 1. æ‰“å¼€å¹¶æå– PDF æ–‡æœ¬
        doc = fitz.open(path)
        for page_num, page in enumerate(doc):
            full_text += page.get_text()
            if len(full_text) > max_chars:
                full_text = full_text[:max_chars]
                logging.info(f"PDF '{path}' å†…å®¹å·²æˆªæ–­è‡³ {max_chars} å­—ç¬¦ã€‚")
                break
        doc.close()

        source_excerpt = full_text[:500] + "..." if full_text else ""
        total_length = len(full_text)

        if not full_text.strip():
            logging.warning(f"PDF æ–‡ä»¶ '{path}' ä¸­æœªæ‰¾åˆ°å¯ç”¨æ–‡æœ¬ã€‚")
            return {
                "summary": "", 
                "error": "PDF æ–‡ä»¶ä¸­æœªæ‰¾åˆ°å¯ç”¨æ–‡æœ¬", 
                "source_excerpt": source_excerpt, 
                "total_length": total_length
            }

        # 2. æ„é€ æ‘˜è¦æç¤º
        prompt = f"""
        You are an academic assistant specializing in research paper analysis.
        Summarize the following academic text into a comprehensive but concise analysis (around 300-400 words in Chinese).
        Focus on:
        1. ç ”ç©¶ç›®æ ‡å’Œé—®é¢˜
        2. ä¸»è¦æ–¹æ³•è®º
        3. æ ¸å¿ƒå‘ç°å’Œç»“è®º
        4. ç ”ç©¶è´¡çŒ®å’Œæ„ä¹‰
        
        è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä½¿ç”¨å­¦æœ¯åŒ–çš„è¯­è¨€ã€‚

        Text:
        \"\"\"
        {full_text}
        \"\"\"
        """

        # 3. è°ƒç”¨è¯­è¨€æ¨¡å‹
        llm = ChatOpenAI(
            temperature=0, 
            model="qwen-plus", 
            base_url=base_url, 
            api_key=DASHSCOPE_API_KEY
        )

        logging.info(f"æ­£åœ¨ä¸ºPDFæ–‡ä»¶ '{path}' ç”Ÿæˆæ‘˜è¦ (è¶…æ—¶æ—¶é—´: 120ç§’)...")
        
        summary_content = "" # é»˜è®¤ä¸ºç©ºå€¼

        def llm_call():
            return llm.invoke([HumanMessage(content=prompt)])

        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(llm_call)
            try:
                response = future.result(timeout=120)  # 2åˆ†é’Ÿè¶…æ—¶
                summary_content = response.content.strip()
                logging.info(f"âœ… PDFæ‘˜è¦ç”ŸæˆæˆåŠŸ: {path}")
            except FuturesTimeoutError:
                logging.warning(f"â³ PDFæ‘˜è¦ç”Ÿæˆè¶…æ—¶ (è¶…è¿‡120ç§’): {path}. è¿”å›ç©ºæ‘˜è¦ã€‚")
                summary_content = "" # è¶…æ—¶åˆ™æ‘˜è¦ä¸ºç©ºå­—ç¬¦ä¸²
            except Exception as e_invoke:
                logging.error(f"âŒ PDFæ‘˜è¦ç”Ÿæˆè¿‡ç¨‹ä¸­LLMè°ƒç”¨å¤±è´¥: {path} - {str(e_invoke)}")
                return {
                    "summary": "", 
                    "error": f"LLMè°ƒç”¨å¤±è´¥: {str(e_invoke)}",
                    "source_excerpt": source_excerpt,
                    "total_length": total_length
                }
        
        return {
            "summary": summary_content,
            "source_excerpt": source_excerpt,
            "total_length": total_length
        }

    except Exception as e:
        logging.error(f"âŒ PDFæ‘˜è¦å·¥å…·æ‰§è¡Œå¤±è´¥: {path} - {str(e)}")
        return {
            "summary": "", 
            "error": f"PDFæ‘˜è¦å¤±è´¥: {str(e)}",
            "source_excerpt": source_excerpt, # å³ä½¿å‡ºé”™ï¼Œä¹Ÿå°è¯•è¿”å›å·²æå–çš„éƒ¨åˆ†
            "total_length": total_length
        }


@tool
def generate_gantt_chart_tool(timeline_content: str, research_field: str = "") -> Dict:
    """ç”Ÿæˆé¡¹ç›®ç”˜ç‰¹å›¾çš„å·¥å…·
    
    Args:
        timeline_content: åŒ…å«æ—¶é—´è§„åˆ’çš„æ–‡æœ¬å†…å®¹
        research_field: ç ”ç©¶é¢†åŸŸåç§°ï¼Œç”¨äºå›¾è¡¨æ ‡é¢˜
        
    Returns:
        åŒ…å«Mermaidç”˜ç‰¹å›¾ä»£ç çš„å­—å…¸
    """
    logging.info(f"è°ƒç”¨å·¥å…·ï¼šgenerate_gantt_chart_tool")
    logging.info(f"è¾“å…¥ç ”ç©¶é¢†åŸŸ: {research_field}")
    logging.info(f"è¾“å…¥æ—¶é—´çº¿å†…å®¹é•¿åº¦: {len(timeline_content)} å­—ç¬¦")
    logging.info(f"æ—¶é—´çº¿å†…å®¹å‰200å­—ç¬¦: {timeline_content[:200]}...")
    
    try:
        # æ„é€ ç”˜ç‰¹å›¾ç”Ÿæˆæç¤º
        gantt_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªé¡¹ç›®ç®¡ç†ä¸“å®¶ï¼Œéœ€è¦æ ¹æ®æä¾›çš„ç ”ç©¶æ—¶é—´çº¿å†…å®¹ç”ŸæˆMermaidæ ¼å¼çš„ç”˜ç‰¹å›¾ã€‚

        **ç ”ç©¶é¢†åŸŸï¼š** {research_field}
        
        **æ—¶é—´çº¿å†…å®¹ï¼š**
        {timeline_content}
        
        **è¦æ±‚ï¼š**
        1. ä»”ç»†åˆ†ææ—¶é—´çº¿å†…å®¹ï¼Œæå–å…³é”®çš„é˜¶æ®µã€ä»»åŠ¡å’Œæ—¶é—´èŠ‚ç‚¹
        2. å°†ä»»åŠ¡æŒ‰é€»è¾‘åˆ†ç»„ä¸ºä¸åŒçš„sectionï¼ˆå¦‚ï¼šæ–‡çŒ®è°ƒç ”ã€ç³»ç»Ÿè®¾è®¡ã€å®éªŒè¯„ä¼°ç­‰ï¼‰
        3. ç”Ÿæˆæ ‡å‡†çš„Mermaidç”˜ç‰¹å›¾è¯­æ³•
        4. ä½¿ç”¨åˆç†çš„æ—¥æœŸæ ¼å¼ï¼ˆYYYY-MM-DDï¼‰
        5. æ ¹æ®ä»»åŠ¡çš„é‡è¦æ€§å’Œä¾èµ–å…³ç³»è®¾ç½®çŠ¶æ€ï¼ˆdone, active, æˆ–ä¸è®¾ç½®ï¼‰
        6. ç¡®ä¿æ—¶é—´å®‰æ’åˆç†ï¼Œé¿å…ä»»åŠ¡é‡å å†²çª
        
        **è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š**
        åªè¾“å‡ºçº¯å‡€çš„Mermaidä»£ç ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        ```mermaid
        gantt
            dateFormat  YYYY-MM-DD
            title       [ç ”ç©¶é¡¹ç›®æ ‡é¢˜]
            section [é˜¶æ®µåç§°1]
            [ä»»åŠ¡åç§°1]    :done,   YYYY-MM-DD, YYYY-MM-DD
            [ä»»åŠ¡åç§°2]    :active, YYYY-MM-DD, 30d
            section [é˜¶æ®µåç§°2]
            [ä»»åŠ¡åç§°3]    :        YYYY-MM-DD, 20d
            [ä»»åŠ¡åç§°4]    :        YYYY-MM-DD, 25d
        ```
        
        æ³¨æ„ï¼š
        - ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—ï¼Œåªè¾“å‡ºMermaidä»£ç 
        - ç¡®ä¿è¯­æ³•æ­£ç¡®ï¼Œå¯ä»¥ç›´æ¥æ¸²æŸ“
        - å¦‚æœæ—¶é—´çº¿å†…å®¹ä¸å¤Ÿè¯¦ç»†ï¼Œè¯·åŸºäºå¸¸è§çš„ç ”ç©¶é¡¹ç›®æµç¨‹è¿›è¡Œåˆç†æ¨æ–­
        - å¿…é¡»ä»¥"gantt"å¼€å¤´
        """

        # è°ƒç”¨LLMç”Ÿæˆç”˜ç‰¹å›¾
        llm = ChatOpenAI(
            temperature=0,
            model="qwen-plus",
            base_url=base_url,
            api_key=DASHSCOPE_API_KEY
        )

        logging.info(f"æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆç”˜ç‰¹å›¾...")
        
        response = llm.invoke([HumanMessage(content=gantt_prompt)])
        gantt_content = response.content.strip()
        
        logging.info(f"LLMåŸå§‹å“åº”é•¿åº¦: {len(gantt_content)} å­—ç¬¦")
        logging.info(f"LLMåŸå§‹å“åº”å‰500å­—ç¬¦: {gantt_content[:500]}...")
        
        # æ¸…ç†è¾“å‡ºï¼Œç¡®ä¿åªåŒ…å«Mermaidä»£ç 
        if "```mermaid" in gantt_content:
            start = gantt_content.find("```mermaid") + 10
            end = gantt_content.find("```", start)
            if end != -1:
                gantt_content = gantt_content[start:end].strip()
                logging.info("âœ… æˆåŠŸæå–mermaidä»£ç å—")
        elif "```" in gantt_content:
            start = gantt_content.find("```") + 3
            end = gantt_content.find("```", start)
            if end != -1:
                gantt_content = gantt_content[start:end].strip()
                logging.info("âœ… æˆåŠŸæå–é€šç”¨ä»£ç å—")
        else:
            logging.info("âš ï¸ æœªæ‰¾åˆ°ä»£ç å—æ ‡è®°ï¼Œä½¿ç”¨åŸå§‹å†…å®¹")
        
        # éªŒè¯ç”˜ç‰¹å›¾å†…å®¹
        if not gantt_content.startswith("gantt"):
            if gantt_content.strip():
                gantt_content = "gantt\n" + gantt_content
                logging.info("âš ï¸ æ·»åŠ äº†ç¼ºå¤±çš„'gantt'å¼€å¤´")
            else:
                logging.error("âŒ ç”˜ç‰¹å›¾å†…å®¹ä¸ºç©º")
                return {
                    "gantt_chart": "",
                    "status": "error",
                    "message": "ç”Ÿæˆçš„ç”˜ç‰¹å›¾å†…å®¹ä¸ºç©º"
                }
        
        logging.info(f"âœ… æœ€ç»ˆç”˜ç‰¹å›¾å†…å®¹é•¿åº¦: {len(gantt_content)} å­—ç¬¦")
        logging.info(f"æœ€ç»ˆç”˜ç‰¹å›¾å†…å®¹å‰300å­—ç¬¦: {gantt_content[:300]}...")
        
        return {
            "gantt_chart": gantt_content,
            "status": "success",
            "message": "ç”˜ç‰¹å›¾ç”ŸæˆæˆåŠŸ"
        }
        
    except Exception as e:
        logging.error(f"âŒ ç”˜ç‰¹å›¾ç”Ÿæˆå¤±è´¥: {str(e)}")
        import traceback
        logging.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return {
            "gantt_chart": "",
            "status": "error", 
            "message": f"ç”˜ç‰¹å›¾ç”Ÿæˆå¤±è´¥: {str(e)}"
        }