"""
è¿‡ç¨‹ä¸­æ¶‰åŠåˆ°çš„ä¸€äº›å·¥å…·ï¼Œå·¥å…·ç›¸å…³é…ç½®è§:tools.json
"""
from concurrent.futures import ThreadPoolExecutor, Future, TimeoutError as FuturesTimeoutError
import arxiv
from langchain_core.tools import tool
import logging
import os
from dotenv import load_dotenv
from typing import List, Dict 
from langchain_community.tools import TavilySearchResults
from crossref.restful import Works
from langchain_core.messages import HumanMessage, SystemMessage
import fitz
from langchain_openai import ChatOpenAI


load_dotenv()
TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY")
DASHSCOPE_API_KEY = os.environ.get("DASHSCOPE_API_KEY")
base_url = os.environ.get("DASHSCOPE_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")

@tool
def search_arxiv_papers_tool(query: str, max_results: int = 10, Download: bool = True) -> List[Dict]:
    """æœç´¢å¹¶ä¸‹è½½ArXivè®ºæ–‡çš„å·¥å…·
    
    Args:
        query: æœç´¢å…³é”®è¯
        max_results: æœ€å¤§ç»“æœæ•°é‡ï¼Œé»˜è®¤5ç¯‡
        Download: æ˜¯å¦ä¸‹è½½PDFæ–‡ä»¶
    
    Returns:
        åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        ä»¥åŠå­˜å‚¨åœ¨Papersç›®å½•ä¸‹çš„å‚è€ƒæ–‡çŒ®
    """
    logging.info(f"åœ¨arxivä¸Šæœç´¢:{query}")
    
    try:
        client = arxiv.Client()
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        
        papers_dir = "./Papers"
        if not os.path.exists(papers_dir):
            os.makedirs(papers_dir)
        
        papers = []
        for paper in client.results(search):
            paper_info = {
                "title": paper.title,
                "authors": [author.name for author in paper.authors],
                "summary": paper.summary[:300] + "...",  # æˆªæ–­æ‘˜è¦
                "published": paper.published.strftime("%Y-%m-%d"),
                "pdf_url": paper.pdf_url,
                "categories": paper.categories,
                "arxiv_id": paper.entry_id.split('/')[-1]
            }
            
            if Download:
                try:
                    # ä¸‹è½½PDF - æ”¹è¿›æ–‡ä»¶åå¤„ç†å’Œé”™è¯¯å¤„ç†
                    logging.info(f"æ­£åœ¨ä¸‹è½½è®ºæ–‡ï¼š{paper.title[:50]}...")
                    
                    # æ›´å®‰å…¨çš„æ–‡ä»¶åå¤„ç†
                    import re
                    safe_title = re.sub(r'[^\w\s-]', '', paper.title)  # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
                    safe_title = re.sub(r'[-\s]+', '-', safe_title)    # æ›¿æ¢ç©ºæ ¼å’Œå¤šä¸ªè¿å­—ç¬¦
                    safe_title = safe_title.strip('-')[:40]             # é™åˆ¶é•¿åº¦å¹¶ç§»é™¤é¦–å°¾è¿å­—ç¬¦
                    
                    if not safe_title:  # å¦‚æœæ ‡é¢˜å¤„ç†åä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
                        safe_title = "paper"
                    
                    filename = f"{paper_info['arxiv_id']}_{safe_title}.pdf"
                    full_path = os.path.join(papers_dir, filename)
                    
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                    if os.path.exists(full_path):
                        logging.info(f"è®ºæ–‡å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {filename}")
                        paper_info["local_pdf_path"] = full_path
                    else:
                        # ä½¿ç”¨æ›´ç¨³å®šçš„ä¸‹è½½æ–¹æ³•
                        import time
                        time.sleep(5)  # å¢åŠ ä¸‹è½½é—´éš”æ—¶é—´ï¼Œä¾‹å¦‚5ç§’ï¼Œä»¥å‡å°‘æœåŠ¡å™¨å‹åŠ›
                        
                        paper.download_pdf(dirpath=papers_dir, filename=filename)
                        
                        # éªŒè¯ä¸‹è½½æ˜¯å¦æˆåŠŸ
                        if os.path.exists(full_path) and os.path.getsize(full_path) > 0:
                            paper_info["local_pdf_path"] = full_path
                            logging.info(f"âœ… æˆåŠŸä¸‹è½½: {filename}")
                        else:
                            paper_info["local_pdf_path"] = None
                            logging.warning(f"âŒ ä¸‹è½½å¤±è´¥æˆ–æ–‡ä»¶ä¸ºç©º: {filename}")
                        
                except Exception as e:
                    paper_info["local_pdf_path"] = None
                    logging.warning(f"âŒ ä¸‹è½½è®ºæ–‡å¤±è´¥: {paper.title[:50]}... - é”™è¯¯: {str(e)}")
                    
                    # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œå°è¯•è®°å½•æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                    error_str = str(e).lower()
                    if "timeout" in error_str:
                        logging.warning("å¯èƒ½çš„ç½‘ç»œè¶…æ—¶é—®é¢˜ã€‚")
                    elif "permission" in error_str or "403" in error_str or "forbidden" in error_str:
                        logging.warning("å¯èƒ½çš„æƒé™é—®é¢˜æˆ–è¯·æ±‚è¢«ç¦æ­¢ (403 Forbidden)ã€‚è¿™å¯èƒ½æ˜¯ç”±äºè¯·æ±‚é¢‘ç‡è¿‡é«˜ã€‚")
                    elif "not found" in error_str or "404" in error_str:
                        logging.warning("PDFæ–‡ä»¶å¯èƒ½ä¸å­˜åœ¨ (404 Not Found)ã€‚")
                    elif "bad gateway" in error_str or "502" in error_str:
                        logging.warning("æœåŠ¡å™¨ç«¯é”™è¯¯ (502 Bad Gateway)ã€‚è¿™å¯èƒ½æ˜¯ArXivæœåŠ¡å™¨çš„ä¸´æ—¶é—®é¢˜ã€‚")
            
            papers.append(paper_info)
            
            # é™åˆ¶å¤„ç†æ•°é‡ï¼Œé¿å…è¿‡å¤šè¯·æ±‚
            if len(papers) >= max_results:
                break

        logging.info(f"âœ… ArXivæœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        successful_downloads = len([p for p in papers if p.get("local_pdf_path")])
        logging.info(f"ğŸ“„ æˆåŠŸä¸‹è½½ {successful_downloads} ä¸ªPDFæ–‡ä»¶")
        
        return papers

    except Exception as e:
        logging.error(f"âŒ ArXivæœç´¢å¤±è´¥: {str(e)}")
        return [{"error": f"ArXivæœç´¢å¤±è´¥: {str(e)}"}]


@tool
def search_web_content_tool(query: str) -> List[Dict]:
    """ä½¿ç”¨Tavilyæœç´¢ç½‘ç»œå†…å®¹çš„å·¥å…·
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨
    """
    logging.info(f"æ­£åœ¨ç½‘ç»œæœç´¢:{query}")
    
    try:
        os.environ["TAVILY_API_KEY"] = TAVILY_API_KEY
        tavily_tool = TavilySearchResults(
            max_results=5,
            search_depth="advanced",
            include_answer=True,
            include_raw_content=True
        )
        
        results = tavily_tool.invoke({"query": query})
        return results
    
    except Exception as e:
        return [{"error": f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}"}]


@tool
def search_crossref_papers_tool(query: str, max_results: int = 5) -> List[Dict]:
    """ä½¿ç”¨ CrossRef æœç´¢è®ºæ–‡å…ƒæ•°æ®çš„å·¥å…·

    Args:
        query: å…³é”®è¯æˆ–ä¸»é¢˜
        max_results: è¿”å›ç»“æœæ•°é‡ä¸Šé™ï¼ˆé»˜è®¤5ï¼‰

    Returns:
        åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
    """
    logging.info(f"åœ¨crossrefä¸Šæœç´¢:{query}")
    
    try:
        works = Works()
        search = works.query(query).sort('relevance')

        results = []
        for i, item in enumerate(search):
            if i >= max_results:
                break

            paper_info = {
                "title": item.get("title", ["No title"])[0],
                "authors": [
                    f"{author.get('given', '')} {author.get('family', '')}".strip()
                    for author in item.get("author", [])
                ],
                "doi": item.get("DOI", "N/A"),
                "published": "-".join(str(d) for d in item.get("issued", {}).get("date-parts", [[None]])[0]),
                "publisher": item.get("publisher", "N/A"),
                "journal": item.get("container-title", ["N/A"])[0],
                "url": item.get("URL", "N/A")
            }

            results.append(paper_info)

        return results

    except Exception as e:
        return [{"error": f"CrossRef æœç´¢å¤±è´¥: {str(e)}"}]


@tool
def summarize_pdf(path: str, max_chars: int = 10000) -> Dict:
    """æ€»ç»“PDFæ–‡ä»¶å†…å®¹çš„å·¥å…·
    
    Args:
        path: PDFæ–‡ä»¶è·¯å¾„
        max_chars: æœ€å¤§å­—ç¬¦æ•°é™åˆ¶ï¼Œé»˜è®¤10000
        
    Returns:
        åŒ…å«æ‘˜è¦å’Œæºæ–‡æœ¬ç‰‡æ®µçš„å­—å…¸ã€‚å¦‚æœæ‘˜è¦ç”Ÿæˆè¶…æ—¶æˆ–å¤±è´¥ï¼Œæ‘˜è¦å†…å®¹å°†ä¸ºç©ºå­—ç¬¦ä¸²ã€‚
    """
    logging.info(f"è°ƒç”¨å·¥å…·ï¼šsummarize_pdf:{path}")
    
    full_text = ""
    source_excerpt = ""
    total_length = 0

    try:
        # 1. æ‰“å¼€å¹¶æå– PDF æ–‡æœ¬
        doc = fitz.open(path)
        for page_num, page in enumerate(doc):
            full_text += page.get_text()
            if len(full_text) > max_chars:
                full_text = full_text[:max_chars]
                logging.info(f"PDF '{path}' å†…å®¹å·²æˆªæ–­è‡³ {max_chars} å­—ç¬¦ã€‚")
                break
        doc.close()

        source_excerpt = full_text[:500] + "..." if full_text else ""
        total_length = len(full_text)

        if not full_text.strip():
            logging.warning(f"PDF æ–‡ä»¶ '{path}' ä¸­æœªæ‰¾åˆ°å¯ç”¨æ–‡æœ¬ã€‚")
            return {
                "summary": "", 
                "error": "PDF æ–‡ä»¶ä¸­æœªæ‰¾åˆ°å¯ç”¨æ–‡æœ¬", 
                "source_excerpt": source_excerpt, 
                "total_length": total_length
            }

        # 2. æ„é€ æ‘˜è¦æç¤º
        prompt = f"""
        You are an academic assistant specializing in research paper analysis.
        Summarize the following academic text into a comprehensive but concise analysis (around 300-400 words in Chinese).
        Focus on:
        1. ç ”ç©¶ç›®æ ‡å’Œé—®é¢˜
        2. ä¸»è¦æ–¹æ³•è®º
        3. æ ¸å¿ƒå‘ç°å’Œç»“è®º
        4. ç ”ç©¶è´¡çŒ®å’Œæ„ä¹‰
        
        è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä½¿ç”¨å­¦æœ¯åŒ–çš„è¯­è¨€ã€‚

        Text:
        \"\"\"
        {full_text}
        \"\"\"
        """

        # 3. è°ƒç”¨è¯­è¨€æ¨¡å‹
        llm = ChatOpenAI(
            temperature=0, 
            model="qwen-plus", 
            base_url=base_url, 
            api_key=DASHSCOPE_API_KEY
        )

        logging.info(f"æ­£åœ¨ä¸ºPDFæ–‡ä»¶ '{path}' ç”Ÿæˆæ‘˜è¦ (è¶…æ—¶æ—¶é—´: 120ç§’)...")
        
        summary_content = "" # é»˜è®¤ä¸ºç©ºå€¼

        def llm_call():
            return llm.invoke([HumanMessage(content=prompt)])

        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(llm_call)
            try:
                response = future.result(timeout=120)  # 2åˆ†é’Ÿè¶…æ—¶
                summary_content = response.content.strip()
                logging.info(f"âœ… PDFæ‘˜è¦ç”ŸæˆæˆåŠŸ: {path}")
            except FuturesTimeoutError:
                logging.warning(f"â³ PDFæ‘˜è¦ç”Ÿæˆè¶…æ—¶ (è¶…è¿‡120ç§’): {path}. è¿”å›ç©ºæ‘˜è¦ã€‚")
                summary_content = "" # è¶…æ—¶åˆ™æ‘˜è¦ä¸ºç©ºå­—ç¬¦ä¸²
            except Exception as e_invoke:
                logging.error(f"âŒ PDFæ‘˜è¦ç”Ÿæˆè¿‡ç¨‹ä¸­LLMè°ƒç”¨å¤±è´¥: {path} - {str(e_invoke)}")
                return {
                    "summary": "", 
                    "error": f"LLMè°ƒç”¨å¤±è´¥: {str(e_invoke)}",
                    "source_excerpt": source_excerpt,
                    "total_length": total_length
                }
        
        return {
            "summary": summary_content,
            "source_excerpt": source_excerpt,
            "total_length": total_length
        }

    except Exception as e:
        logging.error(f"âŒ PDFæ‘˜è¦å·¥å…·æ‰§è¡Œå¤±è´¥: {path} - {str(e)}")
        return {
            "summary": "", 
            "error": f"PDFæ‘˜è¦å¤±è´¥: {str(e)}",
            "source_excerpt": source_excerpt, # å³ä½¿å‡ºé”™ï¼Œä¹Ÿå°è¯•è¿”å›å·²æå–çš„éƒ¨åˆ†
            "total_length": total_length
        }